# train_rl_improved.py
# -*- coding: utf-8 -*-
"""
ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (ì•ŒíŠ¸ì½”ì¸ ìµœì í™” ë²„ì „)
- ì½”ì¸ë³„ ë™ì  íŒŒë¼ë¯¸í„° ì ìš©
- ë³€ë™ì„± ê¸°ë°˜ SL/TP
- ì ì‘í˜• ë ˆë²„ë¦¬ì§€
"""
import os
import json
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
import requests
import gym
from gym import spaces
from typing import Tuple, Dict
from enum import IntEnum


###############################################################################
# í™˜ê²½ ì •ì˜
###############################################################################

class Actions(IntEnum):
    """3ê°€ì§€ ì•¡ì…˜"""
    LONG = 0
    SHORT = 1
    CLOSE = 2


class AdaptiveCryptoTradingEnv(gym.Env):
    """ì•ŒíŠ¸ì½”ì¸ ìµœì í™” ê±°ë˜ í™˜ê²½"""

    metadata = {'render.modes': ['human']}

    def __init__(
            self,
            df: pd.DataFrame,
            window_size: int = 30,
            initial_balance: float = 10000,
            base_leverage: int = 10,
            commission: float = 0.0006,
            stop_loss_multiplier: float = 2.0,  # ATR ë°°ìˆ˜
            take_profit_multiplier: float = 3.0,  # ATR ë°°ìˆ˜
            reward_scaling: float = 1e4,
            min_holding_steps: int = 2,  # ë” ì§§ê²Œ
            force_initial_position: bool = True,
            use_dynamic_leverage: bool = True,  # ğŸ”¥ ë™ì  ë ˆë²„ë¦¬ì§€
            use_adaptive_sltp: bool = True,  # ğŸ”¥ ì ì‘í˜• SL/TP
            debug: bool = False
    ):
        super(AdaptiveCryptoTradingEnv, self).__init__()

        self.df = df.reset_index(drop=True)
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.base_leverage = base_leverage
        self.commission = commission
        self.stop_loss_multiplier = stop_loss_multiplier
        self.take_profit_multiplier = take_profit_multiplier
        self.reward_scaling = reward_scaling
        self.min_holding_steps = min_holding_steps
        self.force_initial_position = force_initial_position
        self.use_dynamic_leverage = use_dynamic_leverage
        self.use_adaptive_sltp = use_adaptive_sltp
        self.debug = debug

        self._calculate_features()
        self._calculate_market_regime()

        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(window_size, len(self.feature_columns) + 4),  # +1 for volatility
            dtype=np.float32
        )
        self.action_space = spaces.Discrete(3)

        self.reset()

    def _calculate_features(self):
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ê¸°ì¡´ê³¼ ë™ì¼)"""
        df = self.df.copy()

        df['returns'] = df['close'].pct_change()
        df['volume_norm'] = np.log1p(df['volume'])

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi'] = df['rsi'] / 100

        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        # SMA
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        df['sma_ratio_20'] = (df['close'] - df['sma_20']) / df['sma_20']
        df['sma_ratio_50'] = (df['close'] - df['sma_50']) / df['sma_50']

        # MACD
        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # ATR (ì¤‘ìš”!)
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(window=14).mean()
        df['atr_pct'] = df['atr'] / df['close']

        # ğŸ”¥ ë³€ë™ì„± ì§€í‘œ ì¶”ê°€
        df['volatility'] = df['returns'].rolling(window=20).std()
        df['volatility_norm'] = df['volatility'] / df['volatility'].rolling(window=100).mean()

        # Price changes
        df['price_change_5'] = df['close'].pct_change(5)
        df['price_change_10'] = df['close'].pct_change(10)
        df['price_change_20'] = df['close'].pct_change(20)
        df['volume_change'] = df['volume'].pct_change()

        df = df.fillna(0).replace([np.inf, -np.inf], 0)

        self.feature_columns = [
            'returns', 'volume_norm', 'rsi', 'bb_position',
            'sma_ratio_20', 'sma_ratio_50',
            'macd', 'macd_signal', 'macd_hist',
            'atr_pct', 'price_change_5', 'price_change_10', 'price_change_20',
            'volume_change', 'volatility_norm'  # ğŸ”¥ ì¶”ê°€
        ]

        self.df = df
        self.feature_mean = self.df[self.feature_columns].mean()
        self.feature_std = self.df[self.feature_columns].std() + 1e-8

    def _calculate_market_regime(self):
        """ğŸ”¥ ì‹œì¥ ìƒíƒœ ë¶„ì„ (ë³€ë™ì„± ê¸°ë°˜)"""
        self.avg_volatility = self.df['atr_pct'].mean()
        self.volatility_std = self.df['atr_pct'].std()

        # ë³€ë™ì„± êµ¬ê°„ë³„ ë¶„ë¥˜
        if self.avg_volatility < 0.01:
            self.market_regime = 'low_vol'
        elif self.avg_volatility < 0.03:
            self.market_regime = 'medium_vol'
        else:
            self.market_regime = 'high_vol'

        if self.debug:
            print(f"ğŸ“Š ì‹œì¥ ìƒíƒœ: {self.market_regime.upper()} (ATR: {self.avg_volatility * 100:.2f}%)")

    def _get_dynamic_leverage(self, current_volatility: float) -> int:
        """ğŸ”¥ ë³€ë™ì„± ê¸°ë°˜ ë™ì  ë ˆë²„ë¦¬ì§€"""
        if not self.use_dynamic_leverage:
            return self.base_leverage

        # ë³€ë™ì„±ì´ ë†’ì„ìˆ˜ë¡ ë ˆë²„ë¦¬ì§€ ë‚®ì¶¤
        volatility_ratio = current_volatility / (self.avg_volatility + 1e-8)

        if volatility_ratio > 1.5:  # ê³ ë³€ë™ì„±
            leverage = max(3, self.base_leverage // 3)
        elif volatility_ratio > 1.2:  # ì¤‘ë³€ë™ì„±
            leverage = max(5, self.base_leverage // 2)
        else:  # ì €ë³€ë™ì„±
            leverage = self.base_leverage

        return int(leverage)

    def _get_adaptive_sltp(self, current_atr_pct: float) -> Tuple[float, float]:
        """ğŸ”¥ ATR ê¸°ë°˜ ë™ì  Stop Loss / Take Profit"""
        if not self.use_adaptive_sltp:
            # ê¸°ë³¸ ê³ ì •ê°’
            return 0.05, 0.08

        # ATRì˜ ë°°ìˆ˜ë¡œ ì„¤ì •
        stop_loss = current_atr_pct * self.stop_loss_multiplier
        take_profit = current_atr_pct * self.take_profit_multiplier

        # ìµœì†Œ/ìµœëŒ€ ì œí•œ
        stop_loss = np.clip(stop_loss, 0.02, 0.10)  # 2%~10%
        take_profit = np.clip(take_profit, 0.03, 0.15)  # 3%~15%

        return float(stop_loss), float(take_profit)

    def reset(self) -> np.ndarray:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.balance = self.initial_balance
        self.equity = self.initial_balance
        self.current_step = self.window_size

        self.current_price = self.df.loc[self.current_step, 'close']
        current_atr = self.df.loc[self.current_step, 'atr_pct']
        current_vol = self.df.loc[self.current_step, 'volatility_norm']

        # ğŸ”¥ ë™ì  íŒŒë¼ë¯¸í„° ì„¤ì •
        self.leverage = self._get_dynamic_leverage(current_atr)
        self.stop_loss_pct, self.take_profit_pct = self._get_adaptive_sltp(current_atr)

        self.position = None
        self.entry_price = 0
        self.position_size = 0
        self.entry_balance = 0

        self.total_trades = 0
        self.winning_trades = 0
        self.total_pnl = 0
        self.max_equity = self.initial_balance
        self.max_drawdown = 0

        self.trade_history = []
        self.equity_history = [self.initial_balance]
        self.steps_in_position = 0

        if self.force_initial_position:
            initial_direction = np.random.choice(['long', 'short'])
            self._open_position(initial_direction)
            if self.debug:
                print(f"\nğŸ² ì´ˆê¸° í¬ì§€ì…˜: {initial_direction.upper()} @ {self.current_price:.2f}")
                print(f"   ë ˆë²„ë¦¬ì§€: {self.leverage}x")
                print(f"   SL: {self.stop_loss_pct * 100:.2f}% / TP: {self.take_profit_pct * 100:.2f}%")

        return self._get_observation()

    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ê´€ì°° (ë³€ë™ì„± ì •ë³´ ì¶”ê°€)"""
        start = self.current_step - self.window_size
        end = self.current_step

        obs_data = self.df[self.feature_columns].iloc[start:end].values
        obs_data = (obs_data - self.feature_mean.values) / self.feature_std.values
        obs_data = np.clip(obs_data, -10, 10)

        position_info = np.zeros((self.window_size, 4))  # +1 for current volatility

        current_vol_norm = self.df.loc[self.current_step, 'volatility_norm']
        position_info[:, 3] = current_vol_norm  # ë³€ë™ì„± ì •ë³´

        if self.position == 'long':
            position_info[:, 0] = 1
            position_info[:, 2] = (self.current_price - self.entry_price) / (self.entry_price + 1e-8)
        elif self.position == 'short':
            position_info[:, 1] = 1
            position_info[:, 2] = (self.entry_price - self.current_price) / (self.entry_price + 1e-8)

        obs = np.concatenate([obs_data, position_info], axis=1)
        return obs.astype(np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1

        self.current_price = self.df.loc[self.current_step, 'close']
        current_atr = self.df.loc[self.current_step, 'atr_pct']

        # ğŸ”¥ ë™ì  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ë§¤ ìŠ¤í…ë§ˆë‹¤)
        if self.use_dynamic_leverage:
            self.leverage = self._get_dynamic_leverage(current_atr)
        if self.use_adaptive_sltp:
            self.stop_loss_pct, self.take_profit_pct = self._get_adaptive_sltp(current_atr)

        reward = self._execute_action(action)
        self._update_equity()

        if self.position:
            self.steps_in_position += 1
        else:
            self.steps_in_position = 0

        self.equity_history.append(self.equity)
        self.max_equity = max(self.max_equity, self.equity)
        drawdown = (self.max_equity - self.equity) / self.max_equity
        self.max_drawdown = max(self.max_drawdown, drawdown)

        if self.equity < self.initial_balance * 0.2:
            done = True
            reward -= 5

        obs = self._get_observation()

        info = {
            'equity': self.equity,
            'balance': self.balance,
            'position': self.position,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(1, self.total_trades),
            'max_drawdown': self.max_drawdown,
            'pnl': self.total_pnl,
            'steps_in_position': self.steps_in_position,
            'current_leverage': self.leverage,
            'current_sl': self.stop_loss_pct,
            'current_tp': self.take_profit_pct
        }

        return obs, reward, done, info

    def _execute_action(self, action: int) -> float:
        """í–‰ë™ ì‹¤í–‰ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ë™ì  SL/TP ì‚¬ìš©)"""
        reward = 0

        # Stop Loss / Take Profit (ë™ì ìœ¼ë¡œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©)
        if self.position:
            pnl_pct = self._get_position_pnl_pct()

            if pnl_pct <= -self.stop_loss_pct:
                reward = self._close_position("Stop Loss")
                reward -= 0.3
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ’” Stop Loss ({pnl_pct * 100:.2f}%)")
                return reward * self.reward_scaling

            if pnl_pct >= self.take_profit_pct:
                reward = self._close_position("Take Profit")
                reward += 1.5
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ¯ Take Profit ({pnl_pct * 100:.2f}%)")
                return reward * self.reward_scaling

        # ì•¡ì…˜ ì‹¤í–‰
        if action == Actions.LONG:
            if self.position == 'short':
                reward += self._close_position("Switch to LONG")
            if not self.position:
                self._open_position('long')
                reward += 0.05

        elif action == Actions.SHORT:
            if self.position == 'long':
                reward += self._close_position("Switch to SHORT")
            if not self.position:
                self._open_position('short')
                reward += 0.05

        elif action == Actions.CLOSE:
            if self.position and self.steps_in_position >= self.min_holding_steps:
                reward = self._close_position("Manual Close")
            elif self.position:
                reward -= 0.1

        # í¬ì§€ì…˜ ìœ ì§€ ë³´ìƒ
        if self.position:
            pnl_pct = self._get_position_pnl_pct()
            reward += pnl_pct * 0.2

        return reward * self.reward_scaling

    def _open_position(self, direction: str):
        """í¬ì§€ì…˜ ì˜¤í”ˆ (ë™ì  ë ˆë²„ë¦¬ì§€ ì‚¬ìš©)"""
        self.position = direction
        self.entry_price = self.current_price
        self.entry_balance = self.balance

        margin = self.balance * 0.95
        self.position_size = (margin * self.leverage) / self.current_price

        commission_cost = self.position_size * self.current_price * self.commission
        self.balance -= commission_cost

        if self.debug:
            print(f"   ğŸ“ˆ {direction.upper()} ì§„ì… @ {self.entry_price:.2f} "
                  f"(Leverage: {self.leverage}x, Size: {self.position_size:.4f})")

    def _close_position(self, reason: str = "") -> float:
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if not self.position:
            return 0

        pnl_pct = self._get_position_pnl_pct()
        pnl = self.entry_balance * pnl_pct * self.leverage

        commission_cost = self.position_size * self.current_price * self.commission
        net_pnl = pnl - commission_cost

        self.balance = self.entry_balance + net_pnl
        self.total_pnl += net_pnl

        if net_pnl > 0:
            self.winning_trades += 1

        self.total_trades += 1

        self.trade_history.append({
            'entry_price': self.entry_price,
            'exit_price': self.current_price,
            'position': self.position,
            'pnl': net_pnl,
            'pnl_pct': pnl_pct,
            'holding_time': self.steps_in_position,
            'reason': reason
        })

        if self.debug:
            print(f"   ğŸ“‰ {self.position.upper()} ì²­ì‚° @ {self.current_price:.2f} "
                  f"| PnL: {net_pnl:+.2f} ({pnl_pct * 100:+.2f}%) | {reason}")

        self.position = None
        self.entry_price = 0
        self.position_size = 0
        self.steps_in_position = 0

        return pnl_pct

    def _get_position_pnl_pct(self) -> float:
        """í˜„ì¬ í¬ì§€ì…˜ ì†ìµë¥ """
        if not self.position:
            return 0

        if self.position == 'long':
            return (self.current_price - self.entry_price) / self.entry_price
        else:
            return (self.entry_price - self.current_price) / self.entry_price

    def _update_equity(self):
        """ìì‚° ì—…ë°ì´íŠ¸"""
        unrealized_pnl = 0
        if self.position:
            pnl_pct = self._get_position_pnl_pct()
            unrealized_pnl = self.entry_balance * pnl_pct * self.leverage

        self.equity = self.balance + unrealized_pnl

    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            'final_equity': self.equity,
            'total_return': ((self.equity - self.initial_balance) / self.initial_balance) * 100,
            'total_trades': self.total_trades,
            'winning_trades': self.winning_trades,
            'win_rate': (self.winning_trades / max(1, self.total_trades)) * 100,
            'max_drawdown': self.max_drawdown * 100,
            'total_pnl': self.total_pnl,
            'market_regime': self.market_regime,
            'avg_volatility': self.avg_volatility
        }


###############################################################################
# ë°ì´í„° ë¡œë“œ
###############################################################################

def fetch_bybit_data(symbol: str, interval: str, limit: int = 1000) -> pd.DataFrame:
    """Bybitì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    url = "https://api.bybit.com/v5/market/kline"

    all_data = []
    end_time = None

    while len(all_data) < limit:
        params = {
            "category": "linear",
            "symbol": symbol,
            "interval": interval,
            "limit": 200
        }

        if end_time:
            params["end"] = end_time

        response = requests.get(url, params=params)
        data = response.json()

        if data['retCode'] != 0:
            print(f"âŒ API ì—ëŸ¬: {data['retMsg']}")
            break

        result = data['result']['list']
        if not result:
            break

        all_data.extend(result)
        end_time = int(result[-1][0]) - 1

        if len(result) < 200:
            break

    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data, columns=[
        'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
    ])

    df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')
    for col in ['open', 'high', 'low', 'close', 'volume']:
        df[col] = df[col].astype(float)

    df = df.sort_values('timestamp').reset_index(drop=True)
    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]

    return df


###############################################################################
# ì½œë°±
###############################################################################

class DetailedCallback(BaseCallback):
    """ìƒì„¸ í•™ìŠµ ë¡œê·¸"""

    def __init__(self, verbose=0):
        super(DetailedCallback, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        if self.locals.get('dones')[0]:
            info = self.locals.get('infos')[0]
            self.episode_rewards.append(info.get('equity', 0))
            self.episode_lengths.append(info.get('total_trades', 0))

            if len(self.episode_rewards) % 10 == 0:
                avg_equity = np.mean(self.episode_rewards[-10:])
                avg_trades = np.mean(self.episode_lengths[-10:])
                print(f"   Episodes: {len(self.episode_rewards)} | "
                      f"Avg Equity: ${avg_equity:.2f} | "
                      f"Avg Trades: {avg_trades:.1f}")

        return True


###############################################################################
# ğŸ”¥ ì½”ì¸ë³„ ìµœì  ì„¤ì •
###############################################################################

COIN_CONFIGS = {
    'BTCUSDT': {
        'stop_loss_mult': 2.0,
        'take_profit_mult': 3.0,
        'base_leverage': 10,
        'min_holding': 3,
        'learning_rate': 3e-4,
        'ent_coef': 0.05,
    },
    'ETHUSDT': {
        'stop_loss_mult': 2.5,
        'take_profit_mult': 3.5,
        'base_leverage': 8,
        'min_holding': 2,
        'learning_rate': 3e-4,
        'ent_coef': 0.07,
    },
    'SOLUSDT': {
        'stop_loss_mult': 3.0,  # ë” í° ë³€ë™ì„±
        'take_profit_mult': 4.0,
        'base_leverage': 5,  # ë‚®ì€ ë ˆë²„ë¦¬ì§€
        'min_holding': 2,
        'learning_rate': 5e-4,  # ë” ë¹ ë¥¸ í•™ìŠµ
        'ent_coef': 0.1,  # ë” ë§ì€ íƒí—˜
    },
    'DEFAULT': {  # ê¸°íƒ€ ì•ŒíŠ¸ì½”ì¸
        'stop_loss_mult': 3.0,
        'take_profit_mult': 4.0,
        'base_leverage': 5,
        'min_holding': 2,
        'learning_rate': 5e-4,
        'ent_coef': 0.1,
    }
}


def get_coin_config(symbol: str) -> Dict:
    """ì½”ì¸ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return COIN_CONFIGS.get(symbol, COIN_CONFIGS['DEFAULT'])


###############################################################################
# í•™ìŠµ
###############################################################################

def train_rl_model(
        symbol: str = 'BTCUSDT',
        interval: str = '5',
        total_timesteps: int = 200000,
        save_path: str = 'rl_models_improved',
        debug: bool = False
):
    """ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (ê°œì„  ë²„ì „)"""
    print("\n" + "=" * 80)
    print(f"{'ğŸš€ ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (ì•ŒíŠ¸ì½”ì¸ ìµœì í™” ë²„ì „!)':^80}")
    print("=" * 80)

    # ğŸ”¥ ì½”ì¸ë³„ ì„¤ì • ë¡œë“œ
    config = get_coin_config(symbol)

    print(f"\nì„¤ì •:")
    print(f"   ì‹¬ë³¼: {symbol}")
    print(f"   ê°„ê²©: {interval}ë¶„")
    print(f"   ì´ ìŠ¤í…: {total_timesteps:,}")
    print(f"   ğŸ”¥ ë™ì  ë ˆë²„ë¦¬ì§€: {config['base_leverage']}x (ë³€ë™ì„± ê¸°ë°˜ ì¡°ì •)")
    print(f"   ğŸ”¥ ì ì‘í˜• SL/TP: ATR Ã— {config['stop_loss_mult']}/{config['take_profit_mult']}")
    print(f"   ğŸ”¥ í•™ìŠµë¥ : {config['learning_rate']}")

    df = fetch_bybit_data(symbol, interval, limit=20000)

    if len(df) < 1000:
        print(f"âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(df)}ê°œ")
        return None, None

    train_size = int(len(df) * 0.8)
    train_df = df.iloc[:train_size].reset_index(drop=True)
    test_df = df.iloc[train_size:].reset_index(drop=True)

    print(f"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(test_df)}ê°œ\n")

    # í™˜ê²½ ìƒì„±
    def make_train_env():
        return AdaptiveCryptoTradingEnv(
            df=train_df,
            window_size=30,
            initial_balance=10000,
            base_leverage=config['base_leverage'],
            commission=0.0006,
            stop_loss_multiplier=config['stop_loss_mult'],
            take_profit_multiplier=config['take_profit_mult'],
            min_holding_steps=config['min_holding'],
            force_initial_position=True,
            use_dynamic_leverage=True,
            use_adaptive_sltp=True,
            debug=False
        )

    def make_eval_env():
        return AdaptiveCryptoTradingEnv(
            df=test_df,
            window_size=30,
            initial_balance=10000,
            base_leverage=config['base_leverage'],
            commission=0.0006,
            stop_loss_multiplier=config['stop_loss_mult'],
            take_profit_multiplier=config['take_profit_mult'],
            min_holding_steps=config['min_holding'],
            force_initial_position=True,
            use_dynamic_leverage=True,
            use_adaptive_sltp=True,
            debug=debug
        )

    vec_env = DummyVecEnv([make_train_env])
    vec_eval_env = DummyVecEnv([make_eval_env])

    print("ğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")

    model = PPO(
        "MlpPolicy",
        vec_env,
        learning_rate=config['learning_rate'],
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=config['ent_coef'],  # ğŸ”¥ ì½”ì¸ë³„ íƒí—˜ ê³„ìˆ˜
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        device="cpu",
        tensorboard_log=f"./tensorboard_logs_improved/{symbol}_{interval}min/"
    )

    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")

    os.makedirs(save_path, exist_ok=True)
    eval_callback = EvalCallback(
        vec_eval_env,
        best_model_save_path=f"{save_path}/{symbol}_{interval}min_best/",
        log_path=f"{save_path}/logs/",
        eval_freq=10000,
        deterministic=True,
        render=False,
        verbose=1
    )

    detailed_callback = DetailedCallback(verbose=1)

    print("=" * 80)
    print(f"{'ğŸš€ í•™ìŠµ ì‹œì‘':^80}")
    print("=" * 80)

    start_time = datetime.now()

    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, detailed_callback],
        progress_bar=True
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60

    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.1f}ë¶„)")

    model_path = f"{save_path}/{symbol}_{interval}min_final.zip"
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    # í‰ê°€
    print("\n" + "=" * 80)
    print(f"{'ğŸ“Š ìµœì¢… í‰ê°€':^80}")
    print("=" * 80)

    if debug:
        print("\nğŸ› ë””ë²„ê·¸ ëª¨ë“œ - ê±°ë˜ ë¡œê·¸:\n" + "-" * 80)

    final_eval_env = AdaptiveCryptoTradingEnv(
        df=test_df,
        window_size=30,
        initial_balance=10000,
        base_leverage=config['base_leverage'],
        commission=0.0006,
        stop_loss_multiplier=config['stop_loss_mult'],
        take_profit_multiplier=config['take_profit_mult'],
        min_holding_steps=config['min_holding'],
        force_initial_position=True,
        use_dynamic_leverage=True,
        use_adaptive_sltp=True,
        debug=debug
    )

    obs = final_eval_env.reset()
    done = False

    action_counts = {0: 0, 1: 0, 2: 0}
    leverage_history = []
    sltp_history = []

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        action_counts[int(action)] += 1
        obs, reward, done, info = final_eval_env.step(action)

        # ë™ì  íŒŒë¼ë¯¸í„° ì¶”ì 
        leverage_history.append(info['current_leverage'])
        sltp_history.append((info['current_sl'], info['current_tp']))

    stats = final_eval_env.get_stats()

    print(f"\nê²€ì¦ ë°ì´í„° ì„±ê³¼:")
    print(f"   ìµœì¢… ìì‚°: ${stats['final_equity']:,.2f}")
    print(f"   ì´ ìˆ˜ìµë¥ : {stats['total_return']:+.2f}%")
    print(f"   ì´ ê±°ë˜: {stats['total_trades']}íšŒ â­")
    print(f"   ìŠ¹ë¥ : {stats['win_rate']:.1f}%")
    print(f"   ìµœëŒ€ ë‚™í­: {stats['max_drawdown']:.2f}%")
    print(f"   ì‹œì¥ ìƒíƒœ: {stats['market_regime'].upper()}")

    print(f"\në™ì  íŒŒë¼ë¯¸í„° í†µê³„:")
    avg_leverage = np.mean(leverage_history)
    avg_sl = np.mean([s[0] for s in sltp_history]) * 100
    avg_tp = np.mean([s[1] for s in sltp_history]) * 100
    print(f"   í‰ê·  ë ˆë²„ë¦¬ì§€: {avg_leverage:.1f}x")
    print(f"   í‰ê·  Stop Loss: {avg_sl:.2f}%")
    print(f"   í‰ê·  Take Profit: {avg_tp:.2f}%")

    print(f"\ní–‰ë™ ë¶„í¬:")
    total = sum(action_counts.values())
    print(f"   LONG:  {action_counts[0]} ({action_counts[0] / total * 100:.1f}%)")
    print(f"   SHORT: {action_counts[1]} ({action_counts[1] / total * 100:.1f}%)")
    print(f"   CLOSE: {action_counts[2]} ({action_counts[2] / total * 100:.1f}%)")

    # ìƒíƒœ í‰ê°€
    print(f"\nìƒíƒœ í‰ê°€:")
    if stats['total_trades'] >= 10:
        print(f"   âœ… ê±°ë˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤! ({stats['total_trades']}íšŒ)")
        if stats['win_rate'] > 55:
            print(f"   âœ… ìš°ìˆ˜í•œ ìŠ¹ë¥ ì…ë‹ˆë‹¤!")
        if stats['total_return'] > 10:
            print(f"   âœ… ìš°ìˆ˜í•œ ìˆ˜ìµë¥ ì…ë‹ˆë‹¤!")
    elif stats['total_trades'] >= 5:
        print(f"   âš ï¸  ê±°ë˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì ìŠµë‹ˆë‹¤ ({stats['total_trades']}íšŒ)")
    else:
        print(f"   âŒ ê±°ë˜ê°€ {stats['total_trades']}íšŒë§Œ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

    # ê±°ë˜ ë‚´ì—­
    if final_eval_env.trade_history:
        print(f"\nê±°ë˜ ìƒì„¸:")
        avg_holding = np.mean([t['holding_time'] for t in final_eval_env.trade_history])
        avg_pnl_pct = np.mean([t['pnl_pct'] * 100 for t in final_eval_env.trade_history])

        print(f"   í‰ê·  ë³´ìœ : {avg_holding:.1f} ìŠ¤í…")
        print(f"   í‰ê·  ì†ìµ: {avg_pnl_pct:+.2f}%")

    # ê²°ê³¼ ì €ì¥
    results = {
        'symbol': symbol,
        'interval': interval,
        'total_timesteps': total_timesteps,
        'config': config,
        'test_stats': stats,
        'action_distribution': action_counts,
        'avg_leverage': float(avg_leverage),
        'avg_stop_loss': float(avg_sl),
        'avg_take_profit': float(avg_tp),
        'model_path': model_path,
        'trained_at': datetime.now().isoformat()
    }

    with open(f"{save_path}/{symbol}_{interval}min_results.json", 'w') as f:
        json.dump(results, f, indent=2)

    # ì°¨íŠ¸
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    # Equity
    axes[0, 0].plot(final_eval_env.equity_history, linewidth=2)
    axes[0, 0].axhline(y=10000, color='r', linestyle='--', alpha=0.5)
    axes[0, 0].set_title('Equity Curve', fontweight='bold')
    axes[0, 0].set_xlabel('Steps')
    axes[0, 0].set_ylabel('Equity ($)')
    axes[0, 0].grid(True, alpha=0.3)

    # Actions
    actions = ['LONG', 'SHORT', 'CLOSE']
    colors = ['green', 'red', 'blue']
    axes[0, 1].bar(actions, [action_counts[i] for i in range(3)], color=colors, alpha=0.7)
    axes[0, 1].set_title('Action Distribution', fontweight='bold')
    axes[0, 1].set_ylabel('Count')
    axes[0, 1].grid(True, alpha=0.3, axis='y')

    # Dynamic Leverage
    axes[1, 0].plot(leverage_history, linewidth=1, alpha=0.7)
    axes[1, 0].set_title('Dynamic Leverage Over Time', fontweight='bold')
    axes[1, 0].set_xlabel('Steps')
    axes[1, 0].set_ylabel('Leverage')
    axes[1, 0].grid(True, alpha=0.3)

    # Dynamic SL/TP
    sl_values = [s[0] * 100 for s in sltp_history]
    tp_values = [s[1] * 100 for s in sltp_history]
    axes[1, 1].plot(sl_values, label='Stop Loss %', linewidth=1, alpha=0.7, color='red')
    axes[1, 1].plot(tp_values, label='Take Profit %', linewidth=1, alpha=0.7, color='green')
    axes[1, 1].set_title('Adaptive SL/TP Over Time', fontweight='bold')
    axes[1, 1].set_xlabel('Steps')
    axes[1, 1].set_ylabel('Percentage (%)')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{save_path}/{symbol}_{interval}min_chart.png", dpi=150)
    print(f"\nğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {save_path}/{symbol}_{interval}min_chart.png")

    print("\n" + "=" * 80)
    print(f"{'âœ… ì™„ë£Œ!':^80}")
    print("=" * 80)

    return model, results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--symbol', type=str, default='BTCUSDT')
    parser.add_argument('--interval', type=str, default='5')
    parser.add_argument('--steps', type=int, default=200000)
    parser.add_argument('--save_path', type=str, default='rl_models_improved')
    parser.add_argument('--debug', action='store_true')

    args = parser.parse_args()

    model, results = train_rl_model(
        symbol=args.symbol,
        interval=args.interval,
        total_timesteps=args.steps,
        save_path=args.save_path,
        debug=args.debug
    )

    if results and results['test_stats']['total_trades'] > 0:
        print("\nğŸ‰ ì„±ê³µ!")
        print(f"   ê±°ë˜: {results['test_stats']['total_trades']}íšŒ")
        print(f"   ìŠ¹ë¥ : {results['test_stats']['win_rate']:.1f}%")
        print(f"   ìˆ˜ìµë¥ : {results['test_stats']['total_return']:+.2f}%")
        print(f"   í‰ê·  ë ˆë²„ë¦¬ì§€: {results['avg_leverage']:.1f}x")