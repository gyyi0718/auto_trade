# train_rl_final_standalone.py
# -*- coding: utf-8 -*-
"""
ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (ìµœì¢… ë…ë¦½ ë²„ì „)
- í™˜ê²½ì„ ì§ì ‘ í¬í•¨
- ê±°ë˜ ë³´ì¥
"""
import os
import json
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
import requests
import gym
from gym import spaces
from typing import Tuple, Dict
from enum import IntEnum


###############################################################################
# í™˜ê²½ ì •ì˜ (ì§ì ‘ í¬í•¨)
###############################################################################

class Actions(IntEnum):
    """3ê°€ì§€ ì•¡ì…˜ë§Œ"""
    LONG = 0  # ë¡± í¬ì§€ì…˜
    SHORT = 1  # ìˆ í¬ì§€ì…˜
    CLOSE = 2  # ì²­ì‚°


class CryptoTradingEnv(gym.Env):
    """ì•”í˜¸í™”í ê±°ë˜ í™˜ê²½ - ìµœì¢… ë²„ì „"""

    metadata = {'render.modes': ['human']}

    def __init__(
            self,
            df: pd.DataFrame,
            window_size: int = 30,
            initial_balance: float = 10000,
            leverage: int = 10,
            commission: float = 0.0006,
            stop_loss_pct: float = 0.05,
            take_profit_pct: float = 0.08,
            reward_scaling: float = 1e4,
            min_holding_steps: int = 3,
            force_initial_position: bool = True,
            debug: bool = False
    ):
        super(CryptoTradingEnv, self).__init__()

        self.df = df.reset_index(drop=True)
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.leverage = leverage
        self.commission = commission
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.reward_scaling = reward_scaling
        self.min_holding_steps = min_holding_steps
        self.force_initial_position = force_initial_position
        self.debug = debug

        self._calculate_features()

        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(window_size, len(self.feature_columns) + 3),
            dtype=np.float32
        )
        self.action_space = spaces.Discrete(3)  # ğŸ”¥ 3ê°œë§Œ!

        self.reset()

    def _calculate_features(self):
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        df = self.df.copy()

        df['returns'] = df['close'].pct_change()
        df['volume_norm'] = np.log1p(df['volume'])

        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi'] = df['rsi'] / 100

        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        df['sma_ratio_20'] = (df['close'] - df['sma_20']) / df['sma_20']
        df['sma_ratio_50'] = (df['close'] - df['sma_50']) / df['sma_50']

        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(window=14).mean()
        df['atr_pct'] = df['atr'] / df['close']

        df['price_change_5'] = df['close'].pct_change(5)
        df['price_change_10'] = df['close'].pct_change(10)
        df['price_change_20'] = df['close'].pct_change(20)
        df['volume_change'] = df['volume'].pct_change()

        df = df.fillna(0).replace([np.inf, -np.inf], 0)

        self.feature_columns = [
            'returns', 'volume_norm', 'rsi', 'bb_position',
            'sma_ratio_20', 'sma_ratio_50',
            'macd', 'macd_signal', 'macd_hist',
            'atr_pct', 'price_change_5', 'price_change_10', 'price_change_20',
            'volume_change'
        ]

        self.df = df
        self.feature_mean = self.df[self.feature_columns].mean()
        self.feature_std = self.df[self.feature_columns].std() + 1e-8

    def reset(self) -> np.ndarray:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.balance = self.initial_balance
        self.equity = self.initial_balance
        self.current_step = self.window_size

        # ğŸ”¥ ë²„ê·¸ ìˆ˜ì •: current_price ì„¤ì •!
        self.current_price = self.df.loc[self.current_step, 'close']

        self.position = None
        self.entry_price = 0
        self.position_size = 0
        self.entry_balance = 0

        self.total_trades = 0
        self.winning_trades = 0
        self.total_pnl = 0
        self.max_equity = self.initial_balance
        self.max_drawdown = 0

        self.trade_history = []
        self.equity_history = [self.initial_balance]

        self.steps_in_position = 0

        # ğŸ”¥ ì´ˆê¸° ëœë¤ í¬ì§€ì…˜!
        if self.force_initial_position:
            initial_direction = np.random.choice(['long', 'short'])
            self._open_position(initial_direction)
            if self.debug:
                print(f"\nğŸ² ì´ˆê¸° í¬ì§€ì…˜: {initial_direction.upper()} @ {self.current_price:.2f}")

        return self._get_observation()

    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ê´€ì°°"""
        start = self.current_step - self.window_size
        end = self.current_step

        obs_data = self.df[self.feature_columns].iloc[start:end].values
        obs_data = (obs_data - self.feature_mean.values) / self.feature_std.values
        obs_data = np.clip(obs_data, -10, 10)

        position_info = np.zeros((self.window_size, 3))
        if self.position == 'long':
            position_info[:, 0] = 1
            position_info[:, 2] = (self.current_price - self.entry_price) / (self.entry_price + 1e-8)
        elif self.position == 'short':
            position_info[:, 1] = 1
            position_info[:, 2] = (self.entry_price - self.current_price) / (self.entry_price + 1e-8)

        obs = np.concatenate([obs_data, position_info], axis=1)
        return obs.astype(np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1

        self.current_price = self.df.loc[self.current_step, 'close']

        reward = self._execute_action(action)
        self._update_equity()

        if self.position:
            self.steps_in_position += 1
        else:
            self.steps_in_position = 0

        self.equity_history.append(self.equity)
        self.max_equity = max(self.max_equity, self.equity)
        drawdown = (self.max_equity - self.equity) / self.max_equity
        self.max_drawdown = max(self.max_drawdown, drawdown)

        if self.equity < self.initial_balance * 0.2:
            done = True
            reward -= 5

        obs = self._get_observation()

        info = {
            'equity': self.equity,
            'balance': self.balance,
            'position': self.position,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(1, self.total_trades),
            'max_drawdown': self.max_drawdown,
            'pnl': self.total_pnl,
            'steps_in_position': self.steps_in_position
        }

        return obs, reward, done, info

    def _execute_action(self, action: int) -> float:
        """í–‰ë™ ì‹¤í–‰"""
        reward = 0

        # Stop Loss / Take Profit
        if self.position:
            pnl_pct = self._get_position_pnl_pct()

            if pnl_pct <= -self.stop_loss_pct:
                reward = self._close_position("Stop Loss")
                reward -= 0.3
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ’” Stop Loss ({pnl_pct * 100:.2f}%)")
                return reward * self.reward_scaling

            if pnl_pct >= self.take_profit_pct:
                reward = self._close_position("Take Profit")
                reward += 1.5
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ’° Take Profit ({pnl_pct * 100:.2f}%)")
                return reward * self.reward_scaling

        # ì•¡ì…˜ ì²˜ë¦¬
        if action == Actions.LONG:
            if self.position == 'long':
                pnl = self._get_position_pnl_pct() * self.leverage
                reward = pnl * 0.1
            elif self.position == 'short':
                if self.steps_in_position >= self.min_holding_steps:
                    reward = self._close_position("Switch")
                    reward += self._open_position('long')
                    if self.debug:
                        print(f"Step {self.current_step}: ğŸ”„ SHORT â†’ LONG")
                else:
                    pnl = self._get_position_pnl_pct() * self.leverage
                    reward = pnl * 0.1 - 0.01
            else:
                reward = self._open_position('long')
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ“ˆ OPEN LONG @ {self.current_price:.2f}")

        elif action == Actions.SHORT:
            if self.position == 'short':
                pnl = self._get_position_pnl_pct() * self.leverage
                reward = pnl * 0.1
            elif self.position == 'long':
                if self.steps_in_position >= self.min_holding_steps:
                    reward = self._close_position("Switch")
                    reward += self._open_position('short')
                    if self.debug:
                        print(f"Step {self.current_step}: ğŸ”„ LONG â†’ SHORT")
                else:
                    pnl = self._get_position_pnl_pct() * self.leverage
                    reward = pnl * 0.1 - 0.01
            else:
                reward = self._open_position('short')
                if self.debug:
                    print(f"Step {self.current_step}: ğŸ“‰ OPEN SHORT @ {self.current_price:.2f}")

        elif action == Actions.CLOSE:
            if self.position and self.steps_in_position >= self.min_holding_steps:
                pos_type = self.position.upper()
                reward = self._close_position("Manual")
                if self.debug:
                    print(f"Step {self.current_step}: âœ‚ï¸ CLOSE {pos_type}")
            elif self.position:
                pnl = self._get_position_pnl_pct() * self.leverage
                reward = pnl * 0.1 - 0.01
            else:
                reward = -0.01

        return reward * self.reward_scaling

    def _open_position(self, direction: str) -> float:
        """í¬ì§€ì…˜ ì§„ì…"""
        self.position = direction
        self.entry_price = self.current_price
        self.entry_balance = self.balance
        self.position_size = (self.balance * self.leverage) / self.current_price
        self.steps_in_position = 0

        fee = self.balance * self.commission
        self.balance -= fee

        return -self.commission

    def _close_position(self, reason: str) -> float:
        """í¬ì§€ì…˜ ì²­ì‚°"""
        if not self.position:
            return 0

        if self.position == 'long':
            pnl = (self.current_price - self.entry_price) * self.position_size
        else:
            pnl = (self.entry_price - self.current_price) * self.position_size

        fee = (self.position_size * self.current_price) * self.commission
        net_pnl = pnl - fee

        self.balance += net_pnl

        self.total_trades += 1
        if net_pnl > 0:
            self.winning_trades += 1
        self.total_pnl += net_pnl

        self.trade_history.append({
            'entry_price': self.entry_price,
            'exit_price': self.current_price,
            'direction': self.position,
            'pnl': net_pnl,
            'pnl_pct': net_pnl / self.entry_balance,
            'reason': reason,
            'holding_time': self.steps_in_position
        })

        if self.debug:
            print(f"  â†’ Trade #{self.total_trades}: {self.position.upper()} | "
                  f"PNL: ${net_pnl:.2f} ({net_pnl / self.entry_balance * 100:+.2f}%) | {reason}")

        self.position = None
        self.entry_price = 0
        self.position_size = 0

        reward = net_pnl / self.entry_balance

        if net_pnl > 0 and self.steps_in_position < 20:
            reward *= 1.2

        return reward

    def _get_position_pnl_pct(self) -> float:
        """í¬ì§€ì…˜ ì†ìµë¥ """
        if not self.position:
            return 0

        if self.position == 'long':
            return (self.current_price - self.entry_price) / (self.entry_price + 1e-8)
        else:
            return (self.entry_price - self.current_price) / (self.entry_price + 1e-8)

    def _update_equity(self):
        """Equity ì—…ë°ì´íŠ¸"""
        if self.position:
            unrealized_pnl = self._get_position_pnl_pct() * self.entry_balance * self.leverage
            self.equity = self.balance + unrealized_pnl
        else:
            self.equity = self.balance

    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        if not self.equity_history:
            return {}

        returns = np.diff(self.equity_history) / (np.array(self.equity_history[:-1]) + 1e-8)
        returns = returns[~np.isnan(returns)]

        if len(returns) > 0:
            sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
        else:
            sharpe = 0

        return {
            'final_equity': self.equity,
            'total_return': (self.equity / self.initial_balance - 1) * 100,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(1, self.total_trades) * 100,
            'max_drawdown': self.max_drawdown * 100,
            'sharpe_ratio': sharpe,
            'total_pnl': self.total_pnl
        }


###############################################################################
# í•™ìŠµ ì½”ë“œ
###############################################################################

class DetailedCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ìƒì„¸ ë©”íŠ¸ë¦­ ë¡œê¹…"""

    def __init__(self, verbose=0):
        super(DetailedCallback, self).__init__(verbose)
        self.episode_trades = []
        self.episode_count = 0

    def _on_step(self) -> bool:
        if 'equity' in self.locals['infos'][0]:
            info = self.locals['infos'][0]

            self.logger.record('train/equity', info['equity'])
            self.logger.record('train/steps_in_position', info.get('steps_in_position', 0))

            if info['total_trades'] > 0:
                self.logger.record('train/win_rate', info['win_rate'])
                self.logger.record('train/total_trades', info['total_trades'])

        if self.locals.get('dones', [False])[0]:
            self.episode_count += 1
            if 'total_trades' in self.locals['infos'][0]:
                trades = self.locals['infos'][0]['total_trades']
                self.episode_trades.append(trades)
                self.logger.record('episode/total_trades', trades)

                if len(self.episode_trades) >= 10:
                    avg_trades = np.mean(self.episode_trades[-10:])
                    self.logger.record('episode/avg_trades_last_10', avg_trades)

        return True


def fetch_bybit_data(symbol: str, interval: str, limit: int = 1000) -> pd.DataFrame:
    """ë°”ì´ë¹„íŠ¸ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    url = "https://api.bybit.com/v5/market/kline"

    all_data = []
    end_time = None

    print(f"ğŸ“¥ {symbol} {interval}ë¶„ë´‰ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")

    while len(all_data) < limit:
        params = {
            "category": "linear",
            "symbol": symbol,
            "interval": interval,
            "limit": 200
        }

        if end_time:
            params["end"] = end_time

        response = requests.get(url, params=params)
        data = response.json()

        if data.get("retCode") != 0:
            print(f"âŒ ì˜¤ë¥˜: {data.get('retMsg')}")
            break

        candles = data["result"]["list"]
        if not candles:
            break

        all_data.extend(candles)
        end_time = int(candles[-1][0]) - 1

        if len(all_data) % 1000 == 0:
            print(f"   ë‹¤ìš´ë¡œë“œ: {len(all_data)}/{limit}")

        if len(candles) < 200:
            break

    df = pd.DataFrame(all_data, columns=[
        "timestamp", "open", "high", "low", "close", "volume", "turnover"
    ])

    df["timestamp"] = pd.to_datetime(df["timestamp"].astype(float), unit="ms")
    for col in ["open", "high", "low", "close", "volume"]:
        df[col] = df[col].astype(float)

    df = df.sort_values("timestamp").reset_index(drop=True)

    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìº”ë“¤\n")

    return df


def train_rl_model(
        symbol: str = "BTCUSDT",
        interval: str = "5",
        total_timesteps: int = 200000,
        save_path: str = "rl_models_standalone",
        debug: bool = False
):
    """ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ"""
    print("\n" + "=" * 80)
    print(f"{'ğŸš€ ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (ë…ë¦½ ë²„ì „ - ê±°ë˜ ë³´ì¥!)':^80}")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"   ì‹¬ë³¼: {symbol}")
    print(f"   ê°„ê²©: {interval}ë¶„")
    print(f"   ì´ ìŠ¤í…: {total_timesteps:,}")
    print(f"   ğŸ”¥ ì•¡ì…˜: 3ê°€ì§€ (LONG=0, SHORT=1, CLOSE=2)")
    print(f"   ğŸ”¥ ì´ˆê¸° í¬ì§€ì…˜: ëœë¤ ê°•ì œ ì§„ì…")

    df = fetch_bybit_data(symbol, interval, limit=20000)

    if len(df) < 1000:
        print(f"âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(df)}ê°œ")
        return None, None

    train_size = int(len(df) * 0.8)
    train_df = df.iloc[:train_size].reset_index(drop=True)
    test_df = df.iloc[train_size:].reset_index(drop=True)

    print(f"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(test_df)}ê°œ\n")

    # í™˜ê²½ ìƒì„±
    def make_train_env():
        return CryptoTradingEnv(
            df=train_df,
            window_size=30,
            initial_balance=10000,
            leverage=10,
            commission=0.0006,
            stop_loss_pct=0.05,
            take_profit_pct=0.08,
            min_holding_steps=3,
            force_initial_position=True,
            debug=False
        )

    def make_eval_env():
        return CryptoTradingEnv(
            df=test_df,
            window_size=30,
            initial_balance=10000,
            leverage=10,
            commission=0.0006,
            stop_loss_pct=0.05,
            take_profit_pct=0.08,
            min_holding_steps=3,
            force_initial_position=True,
            debug=debug
        )

    vec_env = DummyVecEnv([make_train_env])
    vec_eval_env = DummyVecEnv([make_eval_env])

    print("ğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")

    model = PPO(
        "MlpPolicy",
        vec_env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.05,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        device="cpu",
        tensorboard_log=f"./tensorboard_logs_standalone/{symbol}_{interval}min/"
    )

    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")

    os.makedirs(save_path, exist_ok=True)
    eval_callback = EvalCallback(
        vec_eval_env,
        best_model_save_path=f"{save_path}/{symbol}_{interval}min_best/",
        log_path=f"{save_path}/logs/",
        eval_freq=10000,
        deterministic=True,
        render=False,
        verbose=1
    )

    detailed_callback = DetailedCallback(verbose=1)

    print("=" * 80)
    print(f"{'ğŸš€ í•™ìŠµ ì‹œì‘':^80}")
    print("=" * 80)

    start_time = datetime.now()

    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, detailed_callback],
        progress_bar=True
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60

    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.1f}ë¶„)")

    model_path = f"{save_path}/{symbol}_{interval}min_final.zip"
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    # í‰ê°€
    print("\n" + "=" * 80)
    print(f"{'ğŸ“Š ìµœì¢… í‰ê°€':^80}")
    print("=" * 80)

    if debug:
        print("\nğŸ› ë””ë²„ê·¸ ëª¨ë“œ - ê±°ë˜ ë¡œê·¸:\n" + "-" * 80)

    # ğŸ”¥ ìƒˆë¡œìš´ í‰ê°€ í™˜ê²½ ìƒì„± (í™•ì‹¤í•˜ê²Œ!)
    final_eval_env = CryptoTradingEnv(
        df=test_df,
        window_size=30,
        initial_balance=10000,
        leverage=10,
        commission=0.0006,
        stop_loss_pct=0.05,
        take_profit_pct=0.08,
        min_holding_steps=3,
        force_initial_position=True,
        debug=debug
    )

    obs = final_eval_env.reset()
    done = False

    action_counts = {0: 0, 1: 0, 2: 0}

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        action_counts[int(action)] += 1  # ğŸ”¥ intë¡œ ë³€í™˜!
        obs, reward, done, info = final_eval_env.step(action)

    stats = final_eval_env.get_stats()

    print(f"\nê²€ì¦ ë°ì´í„° ì„±ê³¼:")
    print(f"   ìµœì¢… ìì‚°: ${stats['final_equity']:,.2f}")
    print(f"   ì´ ìˆ˜ìµë¥ : {stats['total_return']:+.2f}%")
    print(f"   ì´ ê±°ë˜: {stats['total_trades']}íšŒ â­")
    print(f"   ìŠ¹ë¥ : {stats['win_rate']:.1f}%")
    print(f"   ìµœëŒ€ ë‚™í­: {stats['max_drawdown']:.2f}%")

    print(f"\ní–‰ë™ ë¶„í¬ (3ê°€ì§€):")
    total = sum(action_counts.values())
    print(f"   LONG:  {action_counts[0]} ({action_counts[0] / total * 100:.1f}%)")
    print(f"   SHORT: {action_counts[1]} ({action_counts[1] / total * 100:.1f}%)")
    print(f"   CLOSE: {action_counts[2]} ({action_counts[2] / total * 100:.1f}%)")

    # ìƒíƒœ í‰ê°€
    print(f"\nìƒíƒœ í‰ê°€:")
    if stats['total_trades'] >= 10:
        print(f"   âœ… ê±°ë˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤! ({stats['total_trades']}íšŒ)")
        if stats['win_rate'] > 55:
            print(f"   âœ… ìš°ìˆ˜í•œ ìŠ¹ë¥ ì…ë‹ˆë‹¤!")
    elif stats['total_trades'] >= 5:
        print(f"   âš ï¸  ê±°ë˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì ìŠµë‹ˆë‹¤ ({stats['total_trades']}íšŒ)")
    else:
        print(f"   âŒ ê±°ë˜ê°€ {stats['total_trades']}íšŒë§Œ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

    # ê±°ë˜ ë‚´ì—­
    if final_eval_env.trade_history:
        print(f"\nê±°ë˜ ìƒì„¸:")
        avg_holding = np.mean([t['holding_time'] for t in final_eval_env.trade_history])
        avg_pnl_pct = np.mean([t['pnl_pct'] * 100 for t in final_eval_env.trade_history])

        print(f"   í‰ê·  ë³´ìœ : {avg_holding:.1f} ìŠ¤í…")
        print(f"   í‰ê·  ì†ìµ: {avg_pnl_pct:+.2f}%")

    # ê²°ê³¼ ì €ì¥
    results = {
        'symbol': symbol,
        'interval': interval,
        'total_timesteps': total_timesteps,
        'test_stats': stats,
        'action_distribution': action_counts,
        'model_path': model_path,
        'trained_at': datetime.now().isoformat()
    }

    with open(f"{save_path}/{symbol}_{interval}min_results.json", 'w') as f:
        json.dump(results, f, indent=2)

    # ì°¨íŠ¸
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # Equity
    ax1.plot(final_eval_env.equity_history, linewidth=2)
    ax1.axhline(y=10000, color='r', linestyle='--', alpha=0.5)
    ax1.set_title('Equity Curve', fontweight='bold')
    ax1.set_xlabel('Steps')
    ax1.set_ylabel('Equity ($)')
    ax1.grid(True, alpha=0.3)

    # Actions
    actions = ['LONG', 'SHORT', 'CLOSE']
    colors = ['green', 'red', 'blue']
    ax2.bar(actions, [action_counts[i] for i in range(3)], color=colors, alpha=0.7)
    ax2.set_title('Action Distribution', fontweight='bold')
    ax2.set_ylabel('Count')
    ax2.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(f"{save_path}/{symbol}_{interval}min_chart.png", dpi=150)
    print(f"\nğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {save_path}/{symbol}_{interval}min_chart.png")

    print("\n" + "=" * 80)
    print(f"{'âœ… ì™„ë£Œ!':^80}")
    print("=" * 80)

    return model, results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--symbol', type=str, default='BTCUSDT')
    parser.add_argument('--interval', type=str, default='5')
    parser.add_argument('--steps', type=int, default=200000)
    parser.add_argument('--save_path', type=str, default='rl_models_standalone')
    parser.add_argument('--debug', action='store_true')

    args = parser.parse_args()

    model, results = train_rl_model(
        symbol=args.symbol,
        interval=args.interval,
        total_timesteps=args.steps,
        save_path=args.save_path,
        debug=args.debug
    )

    if results and results['test_stats']['total_trades'] > 0:
        print("\nğŸ‰ ì„±ê³µ!")
        print(f"   ê±°ë˜: {results['test_stats']['total_trades']}íšŒ")
        print(f"   ìŠ¹ë¥ : {results['test_stats']['win_rate']:.1f}%")
        print(f"   ìˆ˜ìµë¥ : {results['test_stats']['total_return']:+.2f}%")
