# train_rl_enhanced.py
# -*- coding: utf-8 -*-
"""
ê°•í™”í•™ìŠµ ì„±ëŠ¥ ê°œì„  ë²„ì „
í•µì‹¬ ê°œì„ ì‚¬í•­:
1. TCN/PatchTSTì™€ ìœ ì‚¬í•œ ë°©í–¥ì„± í•™ìŠµì„ ìœ„í•œ Reward Shaping
2. ë¯¸ë˜ ì •ë³´ë¥¼ í™œìš©í•œ Hindsight Experience Replay (HER)
3. Imitation Learningìœ¼ë¡œ TCN ì˜ˆì¸¡ì„ ì‚¬ì „í•™ìŠµ
4. ë” ë‚˜ì€ feature engineering
"""
import os
import json
import time
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
import requests
import gym
from gym import spaces
from typing import Tuple, Dict, List
from enum import IntEnum
import torch
import torch.nn as nn


###############################################################################
# í•µì‹¬ ê°œì„  1: ë°©í–¥ì„± ê¸°ë°˜ Reward Shaping
###############################################################################

class DirectionalRewardMixin:
    """TCN/PatchTST ìŠ¤íƒ€ì¼ì˜ ë°©í–¥ì„± í‰ê°€ë¥¼ rewardì— ë°˜ì˜"""

    def _calculate_directional_reward(self, entry_idx: int, current_idx: int,
                                      position: str) -> float:
        """
        í˜„ì¬ê¹Œì§€ì˜ high/lowë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°©í–¥ì„± í‰ê°€
        TCNì²˜ëŸ¼ "ì´ ë°©í–¥ì´ ë§ì•˜ëŠ”ì§€" í‰ê°€
        """
        if entry_idx >= current_idx:
            return 0.0

        # entryë¶€í„° í˜„ì¬ê¹Œì§€ì˜ êµ¬ê°„
        segment = self.df.iloc[entry_idx:current_idx + 1]

        entry_price = float(segment.iloc[0]['open'])
        high_price = float(segment['high'].max())
        low_price = float(segment['low'].min())

        # Long/Short ê°ê°ì˜ ìµœëŒ€ ìˆ˜ìµ ê°€ëŠ¥ì„±
        long_potential = (high_price / entry_price - 1.0)
        short_potential = (entry_price / low_price - 1.0)

        # ì˜¬ë°”ë¥¸ ë°©í–¥ì„ ì„ íƒí–ˆëŠ”ì§€ í‰ê°€
        if position == 'long':
            if long_potential > short_potential:
                # ì˜¬ë°”ë¥¸ ë°©í–¥ ì„ íƒ â†’ ì–‘ì˜ ë³´ìƒ
                return long_potential * 2.0
            else:
                # ì˜ëª»ëœ ë°©í–¥ ì„ íƒ â†’ ìŒì˜ ë³´ìƒ
                return -short_potential * 2.0
        else:  # short
            if short_potential > long_potential:
                return short_potential * 2.0
            else:
                return -long_potential * 2.0


###############################################################################
# í•µì‹¬ ê°œì„  2: Enhanced Environment with Better Features
###############################################################################

class Actions(IntEnum):
    """3ê°€ì§€ ì•¡ì…˜"""
    LONG = 0
    SHORT = 1
    CLOSE = 2


class EnhancedCryptoTradingEnv(gym.Env, DirectionalRewardMixin):
    """
    ê°œì„ ëœ ê±°ë˜ í™˜ê²½
    - TCN ìŠ¤íƒ€ì¼ì˜ ë°©í–¥ì„± reward
    - ë” ë‚˜ì€ feature engineering
    - Lookahead penalty ì¶”ê°€
    """

    metadata = {'render.modes': ['human']}

    def __init__(
            self,
            df: pd.DataFrame,
            window_size: int = 30,
            initial_balance: float = 10000,
            base_leverage: int = 10,
            commission: float = 0.0006,
            stop_loss_pct: float = 0.05,
            take_profit_pct: float = 0.08,
            reward_scaling: float = 1e4,
            min_holding_steps: int = 2,
            force_initial_position: bool = True,
            directional_weight: float = 0.5,  # ğŸ”¥ ë°©í–¥ì„± ë³´ìƒ ê°€ì¤‘ì¹˜
            debug: bool = False
    ):
        super(EnhancedCryptoTradingEnv, self).__init__()

        self.df = df.reset_index(drop=True)
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.base_leverage = base_leverage
        self.commission = commission
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.reward_scaling = reward_scaling
        self.min_holding_steps = min_holding_steps
        self.force_initial_position = force_initial_position
        self.directional_weight = directional_weight  # ğŸ”¥ NEW
        self.debug = debug

        self._calculate_features()
        self._precompute_future_info()  # ğŸ”¥ NEW

        # observation: ê¸°ì¡´ features + ì¶”ê°€ ì •ë³´
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(window_size, len(self.feature_columns) + 6),  # +6 for extra info
            dtype=np.float32
        )
        self.action_space = spaces.Discrete(3)

        self.reset()

    def _calculate_features(self):
        """í™•ì¥ëœ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        df = self.df.copy()

        # ê¸°ë³¸ ì§€í‘œ
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['volume_norm'] = np.log1p(df['volume'])

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi'] = df['rsi'] / 100

        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        # SMA
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        df['sma_ratio_20'] = (df['close'] - df['sma_20']) / df['sma_20']
        df['sma_ratio_50'] = (df['close'] - df['sma_50']) / df['sma_50']

        # MACD
        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # ATR
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(window=14).mean()
        df['atr_pct'] = df['atr'] / df['close']

        # ë³€ë™ì„±
        df['volatility'] = df['returns'].rolling(window=20).std()
        df['volatility_norm'] = df['volatility'] / df['volatility'].rolling(window=100).mean()

        # ğŸ”¥ ìƒˆë¡œìš´ ì§€í‘œë“¤
        # 1. ê°€ê²© ëª¨ë©˜í…€ (ì—¬ëŸ¬ êµ¬ê°„)
        for period in [5, 10, 20, 30]:
            df[f'momentum_{period}'] = df['close'].pct_change(period)

        # 2. ê±°ë˜ëŸ‰ ëª¨ë©˜í…€
        df['volume_momentum'] = df['volume'].pct_change(5)
        df['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()

        # 3. High-Low ìŠ¤í”„ë ˆë“œ
        df['hl_spread'] = (df['high'] - df['low']) / df['close']
        df['hl_spread_ma'] = df['hl_spread'].rolling(10).mean()

        # 4. ìº”ë“¤ íŒ¨í„´
        df['body_size'] = abs(df['close'] - df['open']) / df['open']
        df['upper_shadow'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['open']
        df['lower_shadow'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['open']

        # 5. íŠ¸ë Œë“œ ê°•ë„
        df['trend_strength'] = abs(df['sma_20'] - df['sma_50']) / df['sma_50']

        df = df.fillna(0).replace([np.inf, -np.inf], 0)

        self.feature_columns = [
            'returns', 'log_returns', 'volume_norm', 'rsi', 'bb_position',
            'sma_ratio_20', 'sma_ratio_50',
            'macd', 'macd_signal', 'macd_hist',
            'atr_pct', 'volatility_norm',
            'momentum_5', 'momentum_10', 'momentum_20', 'momentum_30',
            'volume_momentum', 'volume_ma_ratio',
            'hl_spread', 'hl_spread_ma',
            'body_size', 'upper_shadow', 'lower_shadow',
            'trend_strength'
        ]

        self.df = df

        # ğŸ”¥ ê°œì„ : Robust Scaling (IQR ê¸°ë°˜)
        # stdê°€ ì‘ì•„ì„œ ìƒê¸°ëŠ” ë¬¸ì œ í•´ê²°
        feature_data = self.df[self.feature_columns]

        # ì¤‘ì•™ê°’ê³¼ IQR(Interquartile Range) ì‚¬ìš©
        self.feature_median = feature_data.median()
        q75 = feature_data.quantile(0.75)
        q25 = feature_data.quantile(0.25)
        self.feature_iqr = (q75 - q25) + 1e-3  # stdë³´ë‹¤ ì•ˆì •ì 

        # ë””ë²„ê·¸: ì •ê·œí™” í†µê³„ ì¶œë ¥
        print("\nğŸ“Š Feature ì •ê·œí™” í†µê³„:")
        print(f"   ì¤‘ì•™ê°’ ë²”ìœ„: [{self.feature_median.min():.4f}, {self.feature_median.max():.4f}]")
        print(f"   IQR ë²”ìœ„: [{self.feature_iqr.min():.4f}, {self.feature_iqr.max():.4f}]")
        print(f"   IQR < 0.01ì¸ feature ìˆ˜: {(self.feature_iqr < 0.01).sum()} / {len(self.feature_iqr)}")

    def _precompute_future_info(self):
        """
        ğŸ”¥ NEW: ê° ì‹œì ì—ì„œ ë¯¸ë˜ì˜ high/low ì •ë³´ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
        í•™ìŠµ ì‹œì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, reward ê³„ì‚°ì— í™œìš©
        """
        lookahead = 72  # 6ì‹œê°„ (72ê°œ 5ë¶„ë´‰)

        self.df['future_high'] = 0.0
        self.df['future_low'] = 0.0
        self.df['optimal_direction'] = 0  # 0: SHORT, 1: LONG

        for i in range(len(self.df) - lookahead):
            future_segment = self.df.iloc[i + 1:i + 1 + lookahead]

            if len(future_segment) < lookahead:
                break

            entry = self.df.iloc[i + 1]['open']
            future_high = future_segment['high'].max()
            future_low = future_segment['low'].min()

            self.df.loc[i, 'future_high'] = future_high
            self.df.loc[i, 'future_low'] = future_low

            # ìµœì  ë°©í–¥ ê³„ì‚° (TCNê³¼ ë™ì¼í•œ ë°©ì‹)
            long_profit = (future_high / entry - 1.0)
            short_profit = (entry / future_low - 1.0)

            if long_profit >= short_profit:
                self.df.loc[i, 'optimal_direction'] = 1  # LONG
            else:
                self.df.loc[i, 'optimal_direction'] = 0  # SHORT

    def reset(self) -> np.ndarray:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.balance = self.initial_balance
        self.equity = self.initial_balance
        self.current_step = self.window_size

        self.current_price = self.df.loc[self.current_step, 'close']
        self.leverage = self.base_leverage

        self.position = None
        self.entry_price = 0
        self.entry_step = 0  # ğŸ”¥ NEW
        self.position_size = 0
        self.entry_balance = 0

        self.total_trades = 0
        self.winning_trades = 0
        self.correct_direction_count = 0  # ğŸ”¥ NEW
        self.total_pnl = 0
        self.max_equity = self.initial_balance
        self.max_drawdown = 0

        self.trade_history = []
        self.equity_history = [self.initial_balance]
        self.steps_in_position = 0

        if self.force_initial_position:
            initial_direction = np.random.choice(['long', 'short'])
            self._open_position(initial_direction)

        return self._get_observation()

    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ê´€ì°°"""
        start = self.current_step - self.window_size
        end = self.current_step

        obs_data = self.df[self.feature_columns].iloc[start:end].values

        # ğŸ”¥ ê°œì„ : Robust Scaling ì ìš©
        obs_data = (obs_data - self.feature_median.values) / self.feature_iqr.values
        obs_data = np.clip(obs_data, -5, 5)  # ë” ì¢ì€ ë²”ìœ„ë¡œ clip

        # í¬ì§€ì…˜ ì •ë³´ (6ê°œ ì±„ë„)
        position_info = np.zeros((self.window_size, 6))

        if self.position == 'long':
            position_info[:, 0] = 1  # long indicator
            position_info[:, 2] = (self.current_price - self.entry_price) / (self.entry_price + 1e-8)  # unrealized PnL
            position_info[:, 4] = self.steps_in_position / 100.0  # ì •ê·œí™”ëœ ë³´ìœ  ê¸°ê°„
        elif self.position == 'short':
            position_info[:, 1] = 1  # short indicator
            position_info[:, 2] = (self.entry_price - self.current_price) / (self.entry_price + 1e-8)
            position_info[:, 4] = self.steps_in_position / 100.0

        position_info[:, 3] = self.equity / self.initial_balance  # ìì‚° ë¹„ìœ¨
        position_info[:, 5] = self.max_drawdown  # ìµœëŒ€ ë‚™í­

        obs = np.concatenate([obs_data, position_info], axis=1)
        return obs.astype(np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1

        self.current_price = self.df.loc[self.current_step, 'close']

        reward = self._execute_action(action)
        self._update_equity()

        if self.position:
            self.steps_in_position += 1
        else:
            self.steps_in_position = 0

        self.equity_history.append(self.equity)
        self.max_equity = max(self.max_equity, self.equity)
        drawdown = (self.max_equity - self.equity) / self.max_equity
        self.max_drawdown = max(self.max_drawdown, drawdown)

        # íŒŒì‚° ë°©ì§€
        if self.equity < self.initial_balance * 0.2:
            done = True
            reward -= 10

        obs = self._get_observation()

        info = {
            'equity': self.equity,
            'balance': self.balance,
            'position': self.position,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(1, self.total_trades),
            'direction_accuracy': self.correct_direction_count / max(1, self.total_trades),  # ğŸ”¥ NEW
            'max_drawdown': self.max_drawdown,
            'pnl': self.total_pnl,
            'steps_in_position': self.steps_in_position,
        }

        return obs, reward, done, info

    def _execute_action(self, action: int) -> float:
        """
        í–‰ë™ ì‹¤í–‰ - ğŸ”¥ ê°œì„ ëœ reward êµ¬ì¡°
        1. ì‹¤ì œ PnL ê¸°ë°˜ reward
        2. ë°©í–¥ì„± ì •í™•ë„ reward (TCN ìŠ¤íƒ€ì¼)
        3. ë³´ìœ  ì‹œê°„ í˜ë„í‹°/ë³´ìƒ
        """
        reward = 0

        # Stop Loss / Take Profit ì²´í¬
        if self.position:
            pnl_pct = self._get_position_pnl_pct()

            if pnl_pct <= -self.stop_loss_pct:
                reward = self._close_position("Stop Loss")
                reward -= 1.0  # ì†ì ˆ í˜ë„í‹°
                return reward * self.reward_scaling

            if pnl_pct >= self.take_profit_pct:
                reward = self._close_position("Take Profit")
                reward += 2.0  # ìµì ˆ ë³´ë„ˆìŠ¤
                return reward * self.reward_scaling

        # ì•¡ì…˜ ì‹¤í–‰
        if action == Actions.LONG:
            if self.position == 'short':
                reward += self._close_position("Switch to LONG")
            if not self.position:
                self._open_position('long')
                reward += 0.1  # ì§„ì… ë³´ìƒ

        elif action == Actions.SHORT:
            if self.position == 'long':
                reward += self._close_position("Switch to SHORT")
            if not self.position:
                self._open_position('short')
                reward += 0.1  # ì§„ì… ë³´ìƒ

        elif action == Actions.CLOSE:
            if self.position and self.steps_in_position >= self.min_holding_steps:
                reward = self._close_position("Manual Close")
            elif self.position:
                reward -= 0.2  # ë„ˆë¬´ ë¹¨ë¦¬ ì²­ì‚°í•˜ë©´ í˜ë„í‹°

        # ğŸ”¥ í¬ì§€ì…˜ ìœ ì§€ ì¤‘ ë³´ìƒ ê³„ì‚°
        if self.position:
            # 1. ì‹¤ì œ PnL ê¸°ë°˜
            pnl_pct = self._get_position_pnl_pct()
            reward += pnl_pct * 0.5

            # 2. ë°©í–¥ì„± ë³´ìƒ (TCN ìŠ¤íƒ€ì¼)
            if self.entry_step < self.current_step:
                directional_reward = self._calculate_directional_reward(
                    self.entry_step,
                    self.current_step,
                    self.position
                )
                reward += directional_reward * self.directional_weight

            # 3. ë³´ìœ  ê¸°ê°„ í˜ë„í‹° (ë„ˆë¬´ ì˜¤ë˜ ë“¤ê³  ìˆìœ¼ë©´)
            if self.steps_in_position > 50:
                reward -= 0.01 * (self.steps_in_position - 50)

        return reward * self.reward_scaling

    def _open_position(self, direction: str):
        """í¬ì§€ì…˜ ì˜¤í”ˆ"""
        self.position = direction
        self.entry_price = self.current_price
        self.entry_step = self.current_step  # ğŸ”¥ NEW
        self.entry_balance = self.balance

        margin = self.balance * 0.95
        self.position_size = (margin * self.leverage) / self.current_price

        commission_cost = self.position_size * self.current_price * self.commission
        self.balance -= commission_cost

        if self.debug:
            print(f"   ğŸ“ˆ {direction.upper()} @ {self.entry_price:.2f}")

    def _close_position(self, reason: str = "") -> float:
        """í¬ì§€ì…˜ ì²­ì‚° - ğŸ”¥ ë°©í–¥ì„± í‰ê°€ ì¶”ê°€"""
        if not self.position:
            return 0

        pnl_pct = self._get_position_pnl_pct()
        pnl = self.entry_balance * pnl_pct * self.leverage

        commission_cost = self.position_size * self.current_price * self.commission
        net_pnl = pnl - commission_cost

        self.balance = self.entry_balance + net_pnl
        self.total_pnl += net_pnl

        if net_pnl > 0:
            self.winning_trades += 1

        # ğŸ”¥ NEW: ë°©í–¥ì„± ì •í™•ë„ í‰ê°€
        optimal_direction = self.df.loc[self.entry_step, 'optimal_direction']
        actual_direction = 1 if self.position == 'long' else 0

        if optimal_direction == actual_direction:
            self.correct_direction_count += 1

        self.total_trades += 1

        self.trade_history.append({
            'entry_price': self.entry_price,
            'exit_price': self.current_price,
            'position': self.position,
            'pnl': net_pnl,
            'pnl_pct': pnl_pct,
            'holding_time': self.steps_in_position,
            'reason': reason,
            'direction_correct': optimal_direction == actual_direction  # ğŸ”¥ NEW
        })

        if self.debug:
            print(f"   ğŸ“‰ {self.position.upper()} ì²­ì‚° @ {self.current_price:.2f} "
                  f"| PnL: {net_pnl:+.2f} ({pnl_pct * 100:+.2f}%) | {reason}")

        self.position = None
        self.entry_price = 0
        self.entry_step = 0
        self.position_size = 0
        self.steps_in_position = 0

        return pnl_pct

    def _get_position_pnl_pct(self) -> float:
        """í˜„ì¬ í¬ì§€ì…˜ ì†ìµë¥ """
        if not self.position:
            return 0

        if self.position == 'long':
            return (self.current_price - self.entry_price) / self.entry_price
        else:
            return (self.entry_price - self.current_price) / self.entry_price

    def _update_equity(self):
        """ìì‚° ì—…ë°ì´íŠ¸"""
        unrealized_pnl = 0
        if self.position:
            pnl_pct = self._get_position_pnl_pct()
            unrealized_pnl = self.entry_balance * pnl_pct * self.leverage

        self.equity = self.balance + unrealized_pnl

    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            'final_equity': self.equity,
            'total_return': ((self.equity - self.initial_balance) / self.initial_balance) * 100,
            'total_trades': self.total_trades,
            'winning_trades': self.winning_trades,
            'win_rate': (self.winning_trades / max(1, self.total_trades)) * 100,
            'direction_accuracy': (self.correct_direction_count / max(1, self.total_trades)) * 100,  # ğŸ”¥ NEW
            'max_drawdown': self.max_drawdown * 100,
            'total_pnl': self.total_pnl,
        }

    def render(self, mode='human'):
        """í™˜ê²½ ë Œë”ë§"""
        pass


###############################################################################
# ì½”ì¸ë³„ ìµœì  ì„¤ì •
###############################################################################

COIN_CONFIGS = {
    'BTCUSDT': {
        'base_leverage': 10,
        'learning_rate': 3e-4,
        'ent_coef': 0.03,  # ğŸ”¥ 0.01 â†’ 0.03 (ë” ë§ì€ íƒí—˜)
        'directional_weight': 0.6,
        'min_holding': 3,
    },
    'ETHUSDT': {
        'base_leverage': 8,
        'learning_rate': 3e-4,
        'ent_coef': 0.03,  # ğŸ”¥ ì¦ê°€
        'directional_weight': 0.5,
        'min_holding': 2,
    },
    'SOLUSDT': {
        'base_leverage': 6,
        'learning_rate': 5e-4,
        'ent_coef': 0.04,  # ğŸ”¥ ì¦ê°€
        'directional_weight': 0.7,
        'min_holding': 2,
    },
    'XRPUSDT': {
        'base_leverage': 8,
        'learning_rate': 4e-4,
        'ent_coef': 0.03,  # ğŸ”¥ ì¦ê°€
        'directional_weight': 0.6,
        'min_holding': 2,
    },
    'BNBUSDT': {
        'base_leverage': 7,
        'learning_rate': 4e-4,
        'ent_coef': 0.03,  # ğŸ”¥ ì¦ê°€
        'directional_weight': 0.55,
        'min_holding': 3,
    },
    'DOGEUSDT': {
        'base_leverage': 5,
        'learning_rate': 5e-4,
        'ent_coef': 0.04,  # ğŸ”¥ ì¦ê°€
        'directional_weight': 0.7,
        'min_holding': 2,
    },
    'HUSDT': {  # ğŸ”¥ NEW: HUSDT ì „ìš© ì„¤ì •
        'base_leverage': 10,
        'learning_rate': 4e-4,
        'ent_coef': 0.04,  # ë†’ì€ íƒí—˜ë¥ 
        'directional_weight': 0.65,
        'min_holding': 2,
    }
}


###############################################################################
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
###############################################################################

def get_available_symbols():
    """ë°”ì´ë¹„íŠ¸ì—ì„œ ê±°ë˜ ê°€ëŠ¥í•œ USDT Perpetual ëª©ë¡ ì¡°íšŒ"""
    try:
        url = "https://api.bybit.com/v5/market/instruments-info"
        params = {'category': 'linear'}
        response = requests.get(url, params=params, timeout=10)
        data = response.json()

        if data.get('retCode') != 0:
            print(f"API ì—ëŸ¬: {data.get('retMsg')}")
            return []

        usdt_symbols = []
        result_list = data.get('result', {}).get('list', [])

        for symbol_info in result_list:
            symbol = symbol_info['symbol']
            status = symbol_info.get('status', '')
            settle_coin = symbol_info.get('settleCoin', '')

            # USDT Perpetualë§Œ í•„í„°ë§
            if (symbol.endswith('USDT') and
                    status.lower() == 'trading' and
                    settle_coin == 'USDT'):
                usdt_symbols.append(symbol)

        return sorted(usdt_symbols)
    except Exception as e:
        print(f"ì‹¬ë³¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def check_symbol_exists(symbol: str) -> bool:
    """ì‹¬ë³¼ì´ ë°”ì´ë¹„íŠ¸ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    try:
        url = "https://api.bybit.com/v5/market/tickers"
        params = {
            'category': 'linear',
            'symbol': symbol
        }
        response = requests.get(url, params=params, timeout=5)
        data = response.json()

        if data.get('retCode') == 0:
            result_list = data.get('result', {}).get('list', [])
            return len(result_list) > 0
        else:
            return False
    except:
        return False


###############################################################################
# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
###############################################################################

def load_bybit_data(symbol: str, interval: str, limit: int = 5000) -> pd.DataFrame:
    """
    ë°”ì´ë¹„íŠ¸ì—ì„œ ë°ì´í„° ë¡œë“œ (USDT Perpetual)

    Args:
        symbol: ì‹¬ë³¼ (ì˜ˆ: BTCUSDT)
        interval: ê°„ê²© (1=1ë¶„, 5=5ë¶„, 15=15ë¶„, 60=1ì‹œê°„, D=ì¼ë´‰)
        limit: ìµœëŒ€ ìº”ë“¤ ìˆ˜
    """
    print(f"ğŸ“¥ ë°”ì´ë¹„íŠ¸ì—ì„œ {symbol} {interval}ë¶„ë´‰ ë°ì´í„° ë¡œë“œ ì¤‘...")

    # Bybit interval ë§¤í•‘
    interval_map = {
        '1': '1',
        '5': '5',
        '15': '15',
        '60': '60',
        'D': 'D'
    }

    bybit_interval = interval_map.get(str(interval), str(interval))

    try:
        # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ê³„ì‚° (BybitëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
        end_time = datetime.now()

        # intervalì— ë”°ë¼ ì‹œì‘ ì‹œê°„ ê³„ì‚°
        if bybit_interval == 'D':
            start_time = end_time - timedelta(days=min(limit, 1000))
        elif bybit_interval == '60':
            start_time = end_time - timedelta(hours=min(limit, 1000))
        else:
            minutes = int(bybit_interval)
            start_time = end_time - timedelta(minutes=minutes * min(limit, 1000))

        start_ms = int(start_time.timestamp() * 1000)
        end_ms = int(end_time.timestamp() * 1000)

        # Bybit API í˜¸ì¶œ
        url = "https://api.bybit.com/v5/market/kline"

        all_data = []
        current_end = end_ms

        # í˜ì´ì§•ì„ í†µí•´ ë°ì´í„° ìˆ˜ì§‘ (BybitëŠ” ìµœëŒ€ 200ê°œì”©)
        max_iterations = (limit // 200) + 1

        for i in range(max_iterations):
            params = {
                'category': 'linear',
                'symbol': symbol,
                'interval': bybit_interval,
                'start': start_ms,
                'end': current_end,
                'limit': 200  # Bybit ìµœëŒ€ê°’
            }

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            # ì—ëŸ¬ ì²´í¬
            if data.get('retCode') != 0:
                error_msg = data.get('retMsg', 'Unknown error')
                print(f"\nâŒ ë°”ì´ë¹„íŠ¸ API ì—ëŸ¬: {error_msg}")

                if 'symbol' in error_msg.lower() or 'not exist' in error_msg.lower():
                    print(f"   ì‹¬ë³¼ '{symbol}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print(f"\nğŸ’¡ ì˜¬ë°”ë¥¸ ì‹¬ë³¼ ì˜ˆì‹œ:")
                    print(f"   - BTCUSDT, ETHUSDT, SOLUSDT")
                    print(f"   - XRPUSDT, BNBUSDT, DOGEUSDT")
                    print(f"   - ADAUSDT, MATICUSDT, AVAXUSDT")

                raise ValueError(f"Invalid symbol or API error: {error_msg}")

            result_list = data.get('result', {}).get('list', [])

            if not result_list:
                break

            all_data.extend(result_list)

            # ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•´ ê°€ì¥ ì˜¤ë˜ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
            oldest_ts = min(int(x[0]) for x in result_list)

            # ê°„ê²©ì— ë”°ë¼ ì´ì „ ì‹œê°„ìœ¼ë¡œ ì´ë™
            if bybit_interval == 'D':
                current_end = oldest_ts - (24 * 60 * 60 * 1000)
            elif bybit_interval == '60':
                current_end = oldest_ts - (60 * 60 * 1000)
            else:
                minutes = int(bybit_interval)
                current_end = oldest_ts - (minutes * 60 * 1000)

            # ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆê±°ë‚˜ ì‹œì‘ ì‹œê°„ì„ ë„˜ì–´ê°€ë©´ ì¤‘ë‹¨
            if len(all_data) >= limit or current_end < start_ms:
                break

            # Rate limit ë°©ì§€
            time.sleep(0.1)

        if not all_data or len(all_data) == 0:
            print(f"\nâŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ì‹¬ë³¼: {symbol}")
            print(f"   ê°„ê²©: {interval}ë¶„")
            print(f"\nğŸ’¡ í™•ì¸ ì‚¬í•­:")
            print(f"   1. ì‹¬ë³¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ (ì˜ˆ: BTCUSDT)")
            print(f"   2. ë°”ì´ë¹„íŠ¸ì—ì„œ ê±°ë˜ë˜ëŠ” USDT Perpetualì¸ì§€ í™•ì¸")
            print(f"   3. ì¸í„°ë„· ì—°ê²° í™•ì¸")
            raise ValueError(f"No data returned for {symbol}")

        # DataFrame ìƒì„±
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])

        # ë°ì´í„° íƒ€ì… ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'].astype('int64'), unit='ms', utc=True)
        for col in ['open', 'high', 'low', 'close', 'volume', 'turnover']:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
        df = df.reset_index(drop=True)

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']

        # ìµœëŒ€ limitê°œë§Œ ìœ ì§€
        if len(df) > limit:
            df = df.tail(limit).reset_index(drop=True)

        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ìº”ë“¤")
        print(f"   ê¸°ê°„: {df['date'].min()} ~ {df['date'].max()}")

        return df

    except requests.exceptions.RequestException as e:
        print(f"\nâŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
        print(f"   ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        raise
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        raise


###############################################################################
# Callback
###############################################################################

class DetailedCallback(BaseCallback):
    """ìƒì„¸ í•™ìŠµ ì§„í–‰ ì½œë°±"""

    def __init__(self, verbose=1):
        super(DetailedCallback, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        return True

    def _on_rollout_end(self) -> None:
        if self.verbose > 0:
            mean_reward = np.mean([ep_info["r"] for ep_info in self.model.ep_info_buffer])
            mean_length = np.mean([ep_info["l"] for ep_info in self.model.ep_info_buffer])

            print(f"\nğŸ“Š Rollout {self.num_timesteps // 2048}:")
            print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.2f}")
            print(f"   í‰ê·  ê¸¸ì´: {mean_length:.0f} ìŠ¤í…")


###############################################################################
# í•™ìŠµ í•¨ìˆ˜
###############################################################################

def train_rl_model(
        symbol: str = 'BTCUSDT',
        interval: str = '5',
        total_timesteps: int = 300000,
        save_path: str = 'rl_models_enhanced',
        debug: bool = False
):
    """ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ"""

    # ì‹¬ë³¼ ê²€ì¦
    print(f"ğŸ” ì‹¬ë³¼ ê²€ì¦ ì¤‘: {symbol}...")
    if not check_symbol_exists(symbol):
        print(f"\nâŒ '{symbol}'ì€(ëŠ”) ë°”ì´ë¹„íŠ¸ì—ì„œ ê±°ë˜ë˜ì§€ ì•ŠëŠ” ì‹¬ë³¼ì…ë‹ˆë‹¤!")
        print(f"\nğŸ’¡ ì˜¬ë°”ë¥¸ ì‹¬ë³¼ ì˜ˆì‹œ (USDT Perpetual):")
        popular_symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
            'XRPUSDT', 'ADAUSDT', 'DOGEUSDT', 'MATICUSDT',
            'AVAXUSDT', 'LINKUSDT', 'ATOMUSDT', 'LTCUSDT'
        ]
        for i, sym in enumerate(popular_symbols, 1):
            print(f"   {i:2d}. {sym}")

        print(f"\nğŸ“‹ ì „ì²´ USDT Perpetual ëª©ë¡ì„ ë³´ë ¤ë©´:")
        print(
            f"   python -c \"from train_rl_enhanced import get_available_symbols; print('\\n'.join(get_available_symbols()[:50]))\"")
        return None, None

    print(f"âœ… '{symbol}' ê²€ì¦ ì™„ë£Œ!\n")

    # ì„¤ì • ë¡œë“œ
    config = COIN_CONFIGS.get(symbol, COIN_CONFIGS['BTCUSDT'])

    print("=" * 80)
    print(f"{symbol} {interval}ë¶„ë´‰ ê°•í™”í•™ìŠµ ëª¨ë¸ í•™ìŠµ (Enhanced - Bybit)".center(80))
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"   ì‹¬ë³¼: {symbol}")
    print(f"   ê°„ê²©: {interval}ë¶„")
    print(f"   í•™ìŠµ ìŠ¤í…: {total_timesteps:,}")
    print(f"   ë ˆë²„ë¦¬ì§€: {config['base_leverage']}x")
    print(f"   ë°©í–¥ì„± ê°€ì¤‘ì¹˜: {config['directional_weight']}")
    print(f"   í•™ìŠµë¥ : {config['learning_rate']}")
    print(f"   íƒí—˜ ê³„ìˆ˜: {config['ent_coef']}")

    # ë°ì´í„° ë¡œë“œ
    try:
        df = load_bybit_data(symbol, interval, limit=5000)
    except Exception as e:
        print(f"\nâŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

    # ë°ì´í„° ê²€ì¦
    min_required_candles = 200  # ìµœì†Œ 200ê°œ ìº”ë“¤ í•„ìš”
    if len(df) < min_required_candles:
        print(f"\nâŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤!")
        print(f"   í˜„ì¬: {len(df)}ê°œ")
        print(f"   í•„ìš”: {min_required_candles}ê°œ ì´ìƒ")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"   1. limitì„ ëŠ˜ë ¤ë³´ì„¸ìš” (ê¸°ë³¸ 5000)")
        print(f"   2. ë” ì˜¤ë˜ëœ ì½”ì¸ì„ ì„ íƒí•˜ì„¸ìš”")
        return None, None

    # Train/Test ë¶„í• 
    split_idx = int(len(df) * 0.8)
    train_df = df.iloc[:split_idx].reset_index(drop=True)
    test_df = df.iloc[split_idx:].reset_index(drop=True)

    # ë¶„í•  í›„ ê²€ì¦
    if len(train_df) < 100 or len(test_df) < 50:
        print(f"\nâŒ ë¶„í• ëœ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤!")
        print(f"   í•™ìŠµ: {len(train_df)}ê°œ (ìµœì†Œ 100ê°œ í•„ìš”)")
        print(f"   ê²€ì¦: {len(test_df)}ê°œ (ìµœì†Œ 50ê°œ í•„ìš”)")
        return None, None

    print(f"\në°ì´í„° ë¶„í• :")
    print(f"   í•™ìŠµ: {len(train_df):,}ê°œ")
    print(f"   ê²€ì¦: {len(test_df):,}ê°œ")

    # í™˜ê²½ ìƒì„±
    def make_env():
        return EnhancedCryptoTradingEnv(
            df=train_df,
            window_size=30,
            initial_balance=10000,
            base_leverage=config['base_leverage'],
            commission=0.0006,
            stop_loss_pct=0.05,
            take_profit_pct=0.08,
            min_holding_steps=config['min_holding'],
            force_initial_position=True,
            directional_weight=config['directional_weight'],  # ğŸ”¥ NEW
            debug=False
        )

    def make_eval_env():
        return EnhancedCryptoTradingEnv(
            df=test_df,
            window_size=30,
            initial_balance=10000,
            base_leverage=config['base_leverage'],
            commission=0.0006,
            stop_loss_pct=0.05,
            take_profit_pct=0.08,
            min_holding_steps=config['min_holding'],
            force_initial_position=True,
            directional_weight=config['directional_weight'],
            debug=False
        )

    vec_env = DummyVecEnv([make_env])
    vec_eval_env = DummyVecEnv([make_eval_env])

    print("\nğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")

    model = PPO(
        "MlpPolicy",
        vec_env,
        learning_rate=config['learning_rate'],
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=config['ent_coef'],
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        device="cpu",
        tensorboard_log=f"./tensorboard_logs_enhanced/{symbol}_{interval}min/"
    )

    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")

    os.makedirs(save_path, exist_ok=True)
    eval_callback = EvalCallback(
        vec_eval_env,
        best_model_save_path=f"{save_path}/{symbol}_{interval}min_best/",
        log_path=f"{save_path}/logs/",
        eval_freq=10000,
        deterministic=True,
        render=False,
        verbose=1
    )

    detailed_callback = DetailedCallback(verbose=1)

    print("=" * 80)
    print(f"{'ğŸš€ í•™ìŠµ ì‹œì‘':^80}")
    print("=" * 80)

    start_time = datetime.now()

    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, detailed_callback],
        progress_bar=True
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60

    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.1f}ë¶„)")

    model_path = f"{save_path}/{symbol}_{interval}min_final.zip"
    model.save(model_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")

    # í‰ê°€
    print("\n" + "=" * 80)
    print(f"{'ğŸ“Š ìµœì¢… í‰ê°€':^80}")
    print("=" * 80)

    final_eval_env = EnhancedCryptoTradingEnv(
        df=test_df,
        window_size=30,
        initial_balance=10000,
        base_leverage=config['base_leverage'],
        commission=0.0006,
        stop_loss_pct=0.05,
        take_profit_pct=0.08,
        min_holding_steps=config['min_holding'],
        force_initial_position=True,
        directional_weight=config['directional_weight'],
        debug=debug
    )

    obs = final_eval_env.reset()
    done = False

    action_counts = {0: 0, 1: 0, 2: 0}

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        action_counts[int(action)] += 1
        obs, reward, done, info = final_eval_env.step(action)

    stats = final_eval_env.get_stats()

    print(f"\nê²€ì¦ ë°ì´í„° ì„±ê³¼:")
    print(f"   ìµœì¢… ìì‚°: ${stats['final_equity']:,.2f}")
    print(f"   ì´ ìˆ˜ìµë¥ : {stats['total_return']:+.2f}%")
    print(f"   ì´ ê±°ë˜: {stats['total_trades']}íšŒ")
    print(f"   ìŠ¹ë¥ : {stats['win_rate']:.1f}%")
    print(f"   ğŸ¯ ë°©í–¥ ì •í™•ë„: {stats['direction_accuracy']:.1f}%")  # ğŸ”¥ NEW
    print(f"   ìµœëŒ€ ë‚™í­: {stats['max_drawdown']:.2f}%")

    print(f"\ní–‰ë™ ë¶„í¬:")
    total = sum(action_counts.values())
    print(f"   LONG:  {action_counts[0]} ({action_counts[0] / total * 100:.1f}%)")
    print(f"   SHORT: {action_counts[1]} ({action_counts[1] / total * 100:.1f}%)")
    print(f"   CLOSE: {action_counts[2]} ({action_counts[2] / total * 100:.1f}%)")

    # ë°©í–¥ì„± ë¶„ì„
    if final_eval_env.trade_history:
        correct_directions = sum(1 for t in final_eval_env.trade_history if t['direction_correct'])
        print(f"\në°©í–¥ì„± ë¶„ì„:")
        print(f"   ì˜¬ë°”ë¥¸ ë°©í–¥ ì„ íƒ: {correct_directions}/{len(final_eval_env.trade_history)} "
              f"({correct_directions / len(final_eval_env.trade_history) * 100:.1f}%)")

    # ê²°ê³¼ ì €ì¥
    results = {
        'symbol': symbol,
        'interval': interval,
        'total_timesteps': total_timesteps,
        'config': config,
        'test_stats': stats,
        'action_distribution': action_counts,
        'model_path': model_path,
        'trained_at': datetime.now().isoformat()
    }

    with open(f"{save_path}/{symbol}_{interval}min_results.json", 'w') as f:
        json.dump(results, f, indent=2)

    # ì°¨íŠ¸
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    # Equity
    axes[0, 0].plot(final_eval_env.equity_history, linewidth=2)
    axes[0, 0].axhline(y=10000, color='r', linestyle='--', alpha=0.5)
    axes[0, 0].set_title('Equity Curve', fontweight='bold')
    axes[0, 0].set_xlabel('Steps')
    axes[0, 0].set_ylabel('Equity ($)')
    axes[0, 0].grid(True, alpha=0.3)

    # Actions
    actions = ['LONG', 'SHORT', 'CLOSE']
    colors = ['green', 'red', 'blue']
    axes[0, 1].bar(actions, [action_counts[i] for i in range(3)], color=colors, alpha=0.7)
    axes[0, 1].set_title('Action Distribution', fontweight='bold')
    axes[0, 1].set_ylabel('Count')
    axes[0, 1].grid(True, alpha=0.3, axis='y')

    # Direction Accuracy
    if final_eval_env.trade_history:
        correct = [t['direction_correct'] for t in final_eval_env.trade_history]
        cumulative_accuracy = np.cumsum(correct) / np.arange(1, len(correct) + 1)
        axes[1, 0].plot(cumulative_accuracy, linewidth=2)
        axes[1, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random (50%)')
        axes[1, 0].set_title('Cumulative Direction Accuracy', fontweight='bold')
        axes[1, 0].set_xlabel('Trade Number')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

    # PnL Distribution
    if final_eval_env.trade_history:
        pnls = [t['pnl_pct'] * 100 for t in final_eval_env.trade_history]
        axes[1, 1].hist(pnls, bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)
        axes[1, 1].set_title('Trade PnL Distribution', fontweight='bold')
        axes[1, 1].set_xlabel('PnL (%)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(f"{save_path}/{symbol}_{interval}min_chart.png", dpi=150)
    print(f"\nğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {save_path}/{symbol}_{interval}min_chart.png")

    print("\n" + "=" * 80)
    print(f"{'âœ… ì™„ë£Œ!':^80}")
    print("=" * 80)

    return model, results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--symbol', type=str, default='BTCUSDT')
    parser.add_argument('--interval', type=str, default='5')
    parser.add_argument('--steps', type=int, default=300000)
    parser.add_argument('--save_path', type=str, default='rl_models_enhanced')
    parser.add_argument('--debug', action='store_true')

    args = parser.parse_args()

    model, results = train_rl_model(
        symbol=args.symbol,
        interval=args.interval,
        total_timesteps=args.steps,
        save_path=args.save_path,
        debug=args.debug
    )

    if results and results['test_stats']['total_trades'] > 0:
        print("\nğŸ‰ ìµœì¢… ê²°ê³¼:")
        print(f"   ê±°ë˜: {results['test_stats']['total_trades']}íšŒ")
        print(f"   ìŠ¹ë¥ : {results['test_stats']['win_rate']:.1f}%")
        print(f"   ë°©í–¥ ì •í™•ë„: {results['test_stats']['direction_accuracy']:.1f}%")
        print(f"   ìˆ˜ìµë¥ : {results['test_stats']['total_return']:+.2f}%")