#!/usr/bin/env python3
"""
ğŸ¯ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ
- ì—¬ëŸ¬ ëª¨ë¸ì˜ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ
- ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìŠ¹ë¥  40% + R/R 30% + ìˆ˜ìµ 20% + Sharpe 10%)
- ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse
from typing import Dict, List, Tuple
import joblib

class ModelEvaluator:
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ"""
    
    def __init__(self, models_dir: str = "./models"):
        self.models_dir = Path(models_dir)
        
        # í‰ê°€ ê°€ì¤‘ì¹˜
        self.weights = {
            'win_rate': 0.40,      # ìŠ¹ë¥  40%
            'risk_reward': 0.30,   # ì†ìµë¹„ 30%
            'total_return': 0.20,  # ì´ìˆ˜ìµ 20%
            'sharpe': 0.10         # ìƒ¤í”„ì§€ìˆ˜ 10%
        }
    
    def find_models(self) -> List[Path]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ëª¨ë¸ íŒŒì¼ ì°¾ê¸°"""
        models = []
        
        if not self.models_dir.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.models_dir}")
            return models
        
        # .pkl ë˜ëŠ” .joblib íŒŒì¼ ì°¾ê¸°
        for ext in ['*.pkl', '*.joblib']:
            models.extend(self.models_dir.rglob(ext))
        
        print(f"âœ… {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
        return sorted(models)
    
    def load_model_metadata(self, model_path: Path) -> Dict:
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        meta_path = model_path.with_suffix('.json')
        
        if meta_path.exists():
            with open(meta_path, 'r') as f:
                return json.load(f)
        
        # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        return {
            'model_name': model_path.stem,
            'created_at': datetime.fromtimestamp(model_path.stat().st_mtime).isoformat(),
            'training_days': 30,
            'features': []
        }
    
    def backtest_model(self, model_path: Path, test_data: pd.DataFrame = None) -> Dict:
        """
        ëª¨ë¸ ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        ì‹¤ì œ êµ¬í˜„ ì‹œ test_dataë¡œ ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        ì—¬ê¸°ì„œëŠ” ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
        """
        meta = self.load_model_metadata(model_path)
        
        # ë©”íƒ€ë°ì´í„°ì— ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if 'backtest_results' in meta:
            return meta['backtest_results']
        
        # ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì œëŒ€ë¡œ ë°±í…ŒìŠ¤íŠ¸ í•´ì•¼ í•¨)
        # ì´ ë¶€ë¶„ì€ ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ë¡œì§ìœ¼ë¡œ êµì²´ í•„ìš”
        try:
            model = joblib.load(model_path)
            
            # ì„ì‹œ: ëœë¤ ê²°ê³¼ ìƒì„± (ì‹¤ì œë¡œëŠ” test_dataë¡œ ë°±í…ŒìŠ¤íŠ¸)
            np.random.seed(hash(str(model_path)) % 2**32)
            
            n_trades = np.random.randint(50, 200)
            wins = np.random.randint(int(n_trades * 0.4), int(n_trades * 0.7))
            
            avg_win = np.random.uniform(2.0, 5.0)
            avg_loss = np.random.uniform(0.5, 2.0)
            
            results = {
                'total_trades': n_trades,
                'wins': wins,
                'losses': n_trades - wins,
                'win_rate': wins / n_trades,
                'avg_win': avg_win,
                'avg_loss': avg_loss,
                'risk_reward': avg_win / avg_loss if avg_loss > 0 else 0,
                'total_return': (wins * avg_win - (n_trades - wins) * avg_loss),
                'sharpe_ratio': np.random.uniform(0.5, 2.5),
                'max_drawdown': np.random.uniform(5, 25)
            }
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ {model_path.name}: {e}")
            return None
    
    def calculate_score(self, results: Dict) -> float:
        """
        ì¢…í•© ì ìˆ˜ ê³„ì‚°
        ì ìˆ˜ = ìŠ¹ë¥ Ã—40% + R/RÃ—30% + ìˆ˜ìµÃ—20% + SharpeÃ—10%
        """
        if not results:
            return 0.0
        
        # ê° ì§€í‘œë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        scores = {
            'win_rate': min(results.get('win_rate', 0) * 100, 100),
            'risk_reward': min(results.get('risk_reward', 0) * 20, 100),  # R/R 5 = 100ì 
            'total_return': min(results.get('total_return', 0) * 2, 100),  # ìˆ˜ìµ 50 = 100ì 
            'sharpe': min(results.get('sharpe_ratio', 0) * 40, 100)  # Sharpe 2.5 = 100ì 
        }
        
        # ê°€ì¤‘ í‰ê· 
        total_score = sum(scores[k] * self.weights[k] for k in self.weights)
        
        return round(total_score, 2)
    
    def evaluate_all_models(self, test_data: pd.DataFrame = None) -> pd.DataFrame:
        """ëª¨ë“  ëª¨ë¸ í‰ê°€ ë° ìˆœìœ„í™”"""
        models = self.find_models()
        
        if not models:
            print("âŒ í‰ê°€í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()
        
        results_list = []
        
        print(f"\n{'='*60}")
        print(f"ğŸ” {len(models)}ê°œ ëª¨ë¸ í‰ê°€ ì¤‘...")
        print(f"{'='*60}\n")
        
        for model_path in models:
            print(f"ğŸ“Š í‰ê°€: {model_path.name}")
            
            meta = self.load_model_metadata(model_path)
            backtest = self.backtest_model(model_path, test_data)
            
            if backtest:
                score = self.calculate_score(backtest)
                
                results_list.append({
                    'model_name': model_path.stem,
                    'model_path': str(model_path),
                    'created_at': meta.get('created_at', 'Unknown'),
                    'training_days': meta.get('training_days', 'Unknown'),
                    'total_trades': backtest['total_trades'],
                    'win_rate': f"{backtest['win_rate']*100:.1f}%",
                    'wins': backtest['wins'],
                    'losses': backtest['losses'],
                    'risk_reward': f"{backtest['risk_reward']:.2f}",
                    'avg_win': f"{backtest['avg_win']:.2f}%",
                    'avg_loss': f"{backtest['avg_loss']:.2f}%",
                    'total_return': f"{backtest['total_return']:.2f}%",
                    'sharpe_ratio': f"{backtest['sharpe_ratio']:.2f}",
                    'max_drawdown': f"{backtest['max_drawdown']:.2f}%",
                    'score': score
                })
                
                print(f"   âœ… ì ìˆ˜: {score:.2f} (ìŠ¹ë¥ : {backtest['win_rate']*100:.1f}%)")
            else:
                print(f"   âŒ í‰ê°€ ì‹¤íŒ¨")
        
        # DataFrame ìƒì„± ë° ì •ë ¬
        df = pd.DataFrame(results_list)
        
        if not df.empty:
            df = df.sort_values('score', ascending=False).reset_index(drop=True)
            df.insert(0, 'rank', range(1, len(df) + 1))
        
        return df
    
    def print_comparison_table(self, df: pd.DataFrame):
        """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
        if df.empty:
            print("âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„")
        print(f"{'='*80}\n")
        
        # ì£¼ìš” ì»¬ëŸ¼ë§Œ ì¶œë ¥
        display_cols = ['rank', 'model_name', 'win_rate', 'risk_reward', 
                       'total_return', 'sharpe_ratio', 'score']
        
        print(df[display_cols].to_string(index=False))
        
        print(f"\n{'='*80}")
        print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥: {df.iloc[0]['model_name']} (ì ìˆ˜: {df.iloc[0]['score']})")
        print(f"{'='*80}\n")
    
    def save_results(self, df: pd.DataFrame, output_path: str = "model_comparison.csv"):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        if df.empty:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        output_path = Path(output_path)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")
    
    def get_best_model(self, df: pd.DataFrame) -> Tuple[str, float]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°˜í™˜"""
        if df.empty:
            return None, 0.0
        
        best = df.iloc[0]
        return best['model_path'], best['score']
    
    def compare_two_models(self, model_a: Path, model_b: Path) -> Dict:
        """
        ë‘ ëª¨ë¸ ì§ì ‘ ë¹„êµ (A/B í…ŒìŠ¤íŠ¸ìš©)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ†š ëª¨ë¸ ë¹„êµ")
        print(f"{'='*60}")
        
        results_a = self.backtest_model(model_a)
        results_b = self.backtest_model(model_b)
        
        if not results_a or not results_b:
            print("âŒ ë¹„êµ ì‹¤íŒ¨")
            return None
        
        score_a = self.calculate_score(results_a)
        score_b = self.calculate_score(results_b)
        
        print(f"\nğŸ“Š ëª¨ë¸ A: {model_a.name}")
        print(f"   ìŠ¹ë¥ : {results_a['win_rate']*100:.1f}% | R/R: {results_a['risk_reward']:.2f} | ì ìˆ˜: {score_a:.2f}")
        
        print(f"\nğŸ“Š ëª¨ë¸ B: {model_b.name}")
        print(f"   ìŠ¹ë¥ : {results_b['win_rate']*100:.1f}% | R/R: {results_b['risk_reward']:.2f} | ì ìˆ˜: {score_b:.2f}")
        
        diff = score_b - score_a
        print(f"\n{'='*60}")
        
        if diff > 5:
            print(f"âœ… ëª¨ë¸ Bê°€ {diff:.2f}ì  ìš°ìˆ˜ â†’ êµì²´ ê¶Œì¥")
            winner = 'B'
        elif diff < -5:
            print(f"âœ… ëª¨ë¸ Aê°€ {abs(diff):.2f}ì  ìš°ìˆ˜ â†’ ìœ ì§€ ê¶Œì¥")
            winner = 'A'
        else:
            print(f"âš ï¸ ì°¨ì´ {abs(diff):.2f}ì  (ì„ê³„ê°’ 5ì ) â†’ ìœ ì§€ ê¶Œì¥")
            winner = 'A'
        
        print(f"{'='*60}\n")
        
        return {
            'model_a': str(model_a),
            'model_b': str(model_b),
            'score_a': score_a,
            'score_b': score_b,
            'difference': diff,
            'winner': winner,
            'results_a': results_a,
            'results_b': results_b
        }


def main():
    parser = argparse.ArgumentParser(description='ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ')
    parser.add_argument('--models-dir', type=str, default='./models',
                       help='ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--save', type=str, default='model_comparison.csv',
                       help='ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…')
    parser.add_argument('--compare', type=str, nargs=2, metavar=('MODEL_A', 'MODEL_B'),
                       help='ë‘ ëª¨ë¸ ì§ì ‘ ë¹„êµ (ê²½ë¡œ)')
    
    args = parser.parse_args()
    
    evaluator = ModelEvaluator(args.models_dir)
    
    # ë‘ ëª¨ë¸ ì§ì ‘ ë¹„êµ ëª¨ë“œ
    if args.compare:
        model_a = Path(args.compare[0])
        model_b = Path(args.compare[1])
        
        if not model_a.exists() or not model_b.exists():
            print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return
        
        result = evaluator.compare_two_models(model_a, model_b)
        
        if result:
            # ë¹„êµ ê²°ê³¼ ì €ì¥
            comparison_path = Path(args.save).with_suffix('.json')
            with open(comparison_path, 'w') as f:
                json.dump(result, f, indent=2)
            print(f"âœ… ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")
        
        return
    
    # ì „ì²´ ëª¨ë¸ í‰ê°€ ëª¨ë“œ
    df = evaluator.evaluate_all_models()
    
    if not df.empty:
        evaluator.print_comparison_table(df)
        evaluator.save_results(df, args.save)
        
        best_model, best_score = evaluator.get_best_model(df)
        print(f"ğŸ¯ ì¶”ì²œ ëª¨ë¸: {Path(best_model).name}")
        print(f"   ì ìˆ˜: {best_score:.2f}\n")


if __name__ == "__main__":
    main()